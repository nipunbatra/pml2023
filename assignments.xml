<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>ES661 PML</title>
<link>https://github.com/nipunbatra/pml2023/assignments.html</link>
<atom:link href="https://github.com/nipunbatra/pml2023/assignments.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.3.450</generator>
<lastBuildDate>Sat, 28 Oct 2023 00:00:00 GMT</lastBuildDate>
<item>
  <title>Assignment 4 (released 28 Oct, due 8 Nov 7 PM)</title>
  <link>https://github.com/nipunbatra/pml2023/assignments/4.html</link>
  <description><![CDATA[ 



<section id="instructions" class="level2">
<h2 class="anchored" data-anchor-id="instructions">Instructions</h2>
<ul>
<li>Total Marks: 9</li>
<li>Use torch for the assignment.</li>
<li>For sampling, use torch.distributions and do not use torch.random directly.</li>
<li>The assignment has to be done in groups of two.</li>
<li>The assignment should be a single jupyter notebook.</li>
<li>The results from every question of your assignment should be in visual formats such as plots and tables. Don’t show model’s log directly in Viva. All plots should have labels and legends appropriately. If not done, we may cut some marks for presentation (e.g.&nbsp;10%).</li>
</ul>
</section>
<section id="questions" class="level2">
<h2 class="anchored" data-anchor-id="questions">Questions</h2>
<ol type="1">
<li>[1 mark] Implement Logistic Regression using the Pyro library referring <a href="https://docs.pyro.ai/en/stable/">[1]</a> for guidance. Show both the mean prediction as well as standard deviation in the predictions over the 2d grid. Use NUTS MCMC sampling to sample the posterior. Take 1000 samples for posterior distribution and use 500 samples as burn/warm up. Use the below given dataset.</li>
</ol>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> make_moons</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> train_test_split</span>
<span id="cb1-3"></span>
<span id="cb1-4">X, y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> make_moons(n_samples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, noise<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span>
<span id="cb1-5">X_train, X_test, y_train, y_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(X, y,test_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span></code></pre></div>
<ol start="2" type="1">
<li>[2 marks] Consider the FVC dataset example discussed in the class. Find the notebook link at <a href="(https://github.com/nipunbatra/pml-teaching/blob/master/notebooks/hierarchical_lr_numpyro.ipynb)">[2]</a>. We had only used the train dataset. Now, we want to find out the performance of various models on the test dataset. Use the given dataset and deduce which model works best in terms of error (<code>MAE</code>) and coverage? The base model is Linear Regression by Sklearn (<code>from sklearn.linear_model import LinearRegression</code>). Plot the trace diagrams and posterior distribution. Also plot the predictive posterior distribution with 90% confidence interval.</li>
</ol>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb2-1">x_train, x_test, y_train, y_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(train[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Weeks"</span>], train[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'FVC'</span>], train_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, random_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span></code></pre></div>
<div class="callout callout-style-default callout-important callout-titled" title="Pooled Model Older (wrong) version">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Pooled Model Older (wrong) version
</div>
</div>
<div class="callout-body-container callout-body">
<p> <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Calpha%20&amp;%5Csim%20%5Cmathcal%7BN%7D(0,%20500)%20%5C%5C%0A%5Cbeta%20&amp;%5Csim%20%5Cmathcal%7BN%7D(0,%20500)%20%5C%5C%0A%5Csigma%20&amp;%5Csim%20%5Ctext%7BHalfNormal%7D(100)%0A%5Cend%7Balign*%7D"> <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bequation*%7D%0AFVC_j%20%5Csim%20%5Ctext%7BNormal%7D(%5Calpha%20+%20%5Cbeta%20%5Ccdot%20Week_,%20%5Csigma)%0A%5Cend%7Bequation*%7D"> where <img src="https://latex.codecogs.com/png.latex?j"> is the patient index.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="Pooled Model Newer (corrected) version">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Pooled Model Newer (corrected) version
</div>
</div>
<div class="callout-body-container callout-body">
<p> <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Calpha%20&amp;%5Csim%20%5Cmathcal%7BN%7D(0,%20500)%20%5C%5C%0A%5Cbeta%20&amp;%5Csim%20%5Cmathcal%7BN%7D(0,%20500)%20%5C%5C%0A%5Csigma%20&amp;%5Csim%20%5Ctext%7BHalfNormal%7D(100)%0A%5Cend%7Balign*%7D"> <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bequation*%7D%0AFVC_j%20%5Csim%20%5Ctext%7BNormal%7D(%5Calpha%20+%20%5Cbeta%20%5Ccdot%20Week_j,%20%5Csigma)%0A%5Cend%7Bequation*%7D"> where <img src="https://latex.codecogs.com/png.latex?j"> is the week index.</p>
</div>
</div>
<hr>
<div class="callout callout-style-default callout-tip callout-titled" title="Partially pooled model with the same sigma">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Partially pooled model with the same sigma
</div>
</div>
<div class="callout-body-container callout-body">
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Cmu_%5Calpha%20&amp;%5Csim%20%5Cmathcal%7BN%7D(0,%20500)%20%5C%5C%0A%5Csigma_%5Calpha%20&amp;%5Csim%20%5Ctext%7BHalfNormal%7D(100)%20%5C%5C%0A%5Cmu_%5Cbeta%20&amp;%5Csim%20%5Cmathcal%7BN%7D(0,%203)%20%5C%5C%0A%5Csigma_%5Cbeta%20&amp;%5Csim%20%5Ctext%7BHalfNormal%7D(3)%20%5C%5C%0A%5Calpha_i%20&amp;%5Csim%20%5Cmathcal%7BN%7D(%5Cmu_%5Calpha,%20%5Csigma_%5Calpha)%20%5C%5C%0A%5Cbeta_i%20&amp;%5Csim%20%5Cmathcal%7BN%7D(%5Cmu_%5Cbeta,%20%5Csigma_%5Cbeta)%20%5C%5C%0A%5Csigma%20&amp;%5Csim%20%5Ctext%7BHalfNormal%7D(100)%0A%5Cend%7Balign*%7D"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bequation*%7D%0AFVC_%7Bij%7D%20%5Csim%20%5Ctext%7BNormal%7D(%5Calpha_i%20+%20%5Cbeta_i%20%5Ccdot%20Week_j,%20%5Csigma)%0A%5Cend%7Bequation*%7D"> where <img src="https://latex.codecogs.com/png.latex?i"> is the patient index and <img src="https://latex.codecogs.com/png.latex?j"> is the week index.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="Partially pooled model with the sigma hyperpriors">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Partially pooled model with the sigma hyperpriors
</div>
</div>
<div class="callout-body-container callout-body">
<p> <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Cmu_%5Calpha%20&amp;%5Csim%20%5Cmathcal%7BN%7D(0,%20500)%20%5C%5C%0A%5Csigma_%5Calpha%20&amp;%5Csim%20%5Ctext%7BHalfNormal%7D(100)%20%5C%5C%0A%5Cmu_%5Cbeta%20&amp;%5Csim%20%5Cmathcal%7BN%7D(0,%203)%20%5C%5C%0A%5Csigma_%5Cbeta%20&amp;%5Csim%20%5Ctext%7BHalfNormal%7D(3)%20%5C%5C%0A%5Calpha_i%20&amp;%5Csim%20%5Cmathcal%7BN%7D(%5Cmu_%5Calpha,%20%5Csigma_%5Calpha)%20%5C%5C%0A%5Cbeta_i%20&amp;%5Csim%20%5Cmathcal%7BN%7D(%5Cmu_%5Cbeta,%20%5Csigma_%5Cbeta)%20%5C%5C%0A%5Cgamma_%5Csigma%20&amp;%5Csim%20%5Ctext%7BHalfNormal%7D(30)%20%5C%5C%0A%5Csigma_i%20&amp;%5Csim%20%5Ctext%7BExp%7D(%5Cgamma_%5Csigma)%0A%5Cend%7Balign*%7D"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bequation*%7D%0AFVC_%7Bij%7D%20%5Csim%20%5Ctext%7BNormal%7D(%5Calpha_i%20+%20%5Cbeta_i%20%5Ccdot%20Week_j,%20%5Csigma_i)%0A%5Cend%7Bequation*%7D"> where <img src="https://latex.codecogs.com/png.latex?i"> is the patient index and <img src="https://latex.codecogs.com/png.latex?j"> is the week index.</p>
</div>
</div>
<ol start="3" type="1">
<li>[4 marks] Use your version of following models to reproduce figure 4 from the paper referenced at <a href="http://proceedings.mlr.press/v80/garnelo18a/garnelo18a.pdf">[2]</a>. You can also refer to the notebook in the <a href="https://github.com/nipunbatra/pml-teaching/blob/master/notebooks/hypernet-np.ipynb">course</a>.
<ol type="i">
<li>Hypernet [2 marks]</li>
<li>Neural Processes [2 marks]</li>
</ol></li>
<li>[2 marks] Write the Random walk Metropolis Hastings algorithms from scratch. Take 1000 samples using below given log probs and compare the mean and covariance matrix with hamiltorch’s standard HMC and <a href="https://emcee.readthedocs.io">emcee’s</a> Metropolis Hastings implementation. Use 500 samples as the burn/warm up samples. Also check the relation between acceptance ratio and the sigma of the proposal distribution in your from scratch implementation. Use the log likelihood function given below.</li>
</ol>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch.distributions <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> D</span>
<span id="cb3-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> log_likelihood(omega):</span>
<span id="cb3-3">    omega <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor(omega)</span>
<span id="cb3-4">    mean <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor([<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.</span>])</span>
<span id="cb3-5">    stddev <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor([<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.</span>])</span>
<span id="cb3-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> D.MultivariateNormal(mean, torch.diag(stddev<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)).log_prob(omega).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>()</span></code></pre></div>


</section>

 ]]></description>
  <guid>https://github.com/nipunbatra/pml2023/assignments/4.html</guid>
  <pubDate>Sat, 28 Oct 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Assignment 3 (released 18 Sep, due 27 September 7 PM)</title>
  <link>https://github.com/nipunbatra/pml2023/assignments/3.html</link>
  <description><![CDATA[ 



<section id="instructions" class="level2">
<h2 class="anchored" data-anchor-id="instructions">Instructions</h2>
<ul>
<li>Total marks: 9</li>
<li>Use torch for the assignment</li>
<li>For sampling, use torch.distributions and do not use torch.random directly</li>
<li>For computing log_prob, use torch.distributions and do not use custom formulas</li>
<li>The assignment has to be done in groups of two.</li>
<li>The assignment should be a single Jupyter notebook.</li>
<li>The results from every question of your assignment should be in visual formats such as plots and tables. Don’t show model’s log directly in Viva. All plots should have labels and legends appropriately. If not done, we may cut some marks for presentation (e.g.&nbsp;10%).</li>
<li>To know more about a distribution, just look at the Wikipedia page (table on the right will have everything you need).</li>
</ul>
</section>
<section id="questions" class="level2">
<h2 class="anchored" data-anchor-id="questions">Questions</h2>
<ol type="1">
<li>[2.5 marks] Approximate the normalization constant for standard normal distribution using Monte Carlo integration. Assume that we only know the numerator term: <img src="https://latex.codecogs.com/png.latex?e%5E%5Cfrac%7B-x%5E2%7D%7B2%7D"> and want to find <img src="https://latex.codecogs.com/png.latex?I%20=%20%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7D%20e%5E%5Cfrac%7B-x%5E2%7D%7B2%7D%20dx">: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bequation%7D%0A%5Cint%20f(x)%20p(x)%20dx%20%5Capprox%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20f(x_i)%20%5Ctext%7B%20where%20%7D%20x_i%20%5Csim%20p(x).%0A%5Cend%7Bequation%7D">
<ol type="i">
<li>Take <code>p(x) = Uniform(-a, a)</code>. Choose the following values of <code>a</code> {0.01, 0.02, 0.05, 0.1, 0.5, 1, 2, 3, 5}. Draw 1000 samples for each <code>a</code> and plot the normalizing constant on y-axis for each value of <code>a</code> on x-axis. Draw a horizontal line showing the analytical normalizing constant as ground truth. [1 marks]</li>
<li>Estimate <code>I</code> using Monte Carlo integration for varying number of MC samples {10, 100, 10^3 , 10^4, 10^5} for <code>a=4</code>. For each value of number of MC samples, repeat the experiment 10 times and plot the mean estimate with <code>plt.plot</code> and the standard deviation of the estimate using <code>plt.fill_between</code>. [1 mark]</li>
<li>Repeat (i.) using <code>scipy.integrate.quad</code> and compare it with the estimates obtained in (i.) using a similar plot used in (i.). [0.5 mark]</li>
</ol></li>
<li>[1.5 marks] Inverse CDF sampling for <a href="https://en.wikipedia.org/wiki/Cauchy_distribution">Cauchy distribution</a>:
<ol type="i">
<li>Analytically derive the Inverse CDF from the CDF of the Cauchy distribution. [0.5 mark]</li>
<li>Draw samples from the Cauchy distribution (loc=0, scale=1) with inverse CDF sampling. Use the inverse CDF derived in (i.). While generating samples from the uniform distribution, restrict the samples to be between 0.05 and 0.95 to avoid numerical instability. Verify that drawn samples are correct by plotting the kernel density estimation (empirical pdf) with <code>sns.kdeplot</code> (<code>sns</code> stands for the <a href="https://seaborn.pydata.org/">seaborn</a> library) along with pdf obtained with <code>dist.log_prob</code>, where <code>dist</code> is <code>torch.distributions.Cauchy(loc=0, scale=1)</code> [0.5 mark]</li>
<li>Repeat (ii.) using inverse CDF from <code>torch.distributions.Cauchy</code>. You can access the inverse CDF at <code>dist.icdf</code> where <code>dist</code> is an instance of <code>torch.distributions.Cauchy</code>. [0.5 mark]</li>
</ol></li>
<li>[1.5 marks] Rejection sampling:
<ol type="i">
<li>Sample the unnormalized distribution shown in the code below using rejection sampling. Use Normal(loc=5, scale=5), Uniform(-15, 15), and Laplace distribution(loc=5, scale=5) as the proposal distributions. Report the accepance ratios for each proposal distribution (You may choose suitable Multiplier value (M) while considering the support -15 &lt; x &lt; 15). [1 mark]</li>
<li>Create and compare plots showing the target distribution (<code>taget_pdf</code> function), proposal distribution (pdf via <code>log_prob</code> method), scaled proposal distribution (scaled by M), and pdf of final normalized target distribution (empirical pdf) using <code>sns.kdeplot</code>. [0.5 mark]</li>
</ol></li>
</ol>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch.distributions <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> D</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> target_pdf(x):</span>
<span id="cb1-5">    gaussian_pdf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> D.Normal(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.5</span>).log_prob(x).exp()</span>
<span id="cb1-6">    cauchy_pdf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> D.Cauchy(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>).log_prob(x).exp()</span>
<span id="cb1-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> gaussian_pdf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> cauchy_pdf</span></code></pre></div>
<ol start="4" type="1">
<li>[3.5 marks] Generate the following classification dataset:</li>
</ol>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> make_circles</span>
<span id="cb2-2">X, y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> make_circles(n_samples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, noise<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.02</span>, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span></code></pre></div>
<p>We want to perform classification with the following Neural Network which returns logits for the cross-entropy loss:</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch.nn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> nn</span>
<span id="cb3-2">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Sequential(</span>
<span id="cb3-3">    nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>),</span>
<span id="cb3-4">    nn.ReLU(),</span>
<span id="cb3-5">    nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb3-6">)</span></code></pre></div>
<ol type="i">
<li>What are the total number of parameters in this model? Apply <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BN%7D(0,%201)"> prior on all the parameters of the neural network and find MAP estimate of parameters and plot the predicted probability surface on a dense grid along with data (see <a href="https://machinelearningmastery.com/plot-a-decision-surface-for-machine-learning/">this example</a> for code reference). You may choose the appropriate limits for the grid to cover the data with some additional margin. [0.5 mark]</li>
<li>What is the expected size of the Hessian matrix? Compute Full Hessian and invert it to get the covariance matrix. Visualize the heatmap of this covariance matrix with seaborn or matplotlib. A valid covariance matrix is always positive semi-definite (PSD). Check if the obtained covariance matrix is PSD. [1.5 marks]</li>
<li>As an approximation, we can ignore the off-diagonal elements of Hessian. Why such a matrix might be easier to invert? After inverting and getting the covariance matrix, visualize it with a heatmap and compare it with the Full covariance obtained in (ii). [0.5 mark]</li>
<li>Sample parameters from the posterior distribution. Use these parameters to obtain the predictive posterior with Monte Carlo sampling on a uniform 2d grid. Plot mean and standard deviation surfaces side-by-side with something similar to <code>plt.subplots()</code>. What are your observations? [1 mark]</li>
</ol>
<p>Following functions from <a href="https://github.com/AdamCobb/hamiltorch"><code>hamiltorch</code></a> will become useful while working on this:</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># flatten function returns a 1d tensor of parameters from the model.</span></span>
<span id="cb4-2">flat_params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> hamiltorch.util.flatten(model)</span>
<span id="cb4-3"></span>
<span id="cb4-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># unflatten function returns a list of parameters which has</span></span>
<span id="cb4-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># the same structure as list(model.parameters())</span></span>
<span id="cb4-6">params_list <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> hamiltorch.util.unflatten(model, flat_params)</span>
<span id="cb4-7"></span>
<span id="cb4-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Put back the weights in the model in-place</span></span>
<span id="cb4-9">hamiltorch.util.update_model_params_in_place(model, params_list)</span></code></pre></div>
<!-- 6. BMH data
https://nbviewer.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter1_Introduction/Ch1_Introduction_PyMC3.ipynb -->


</section>

 ]]></description>
  <guid>https://github.com/nipunbatra/pml2023/assignments/3.html</guid>
  <pubDate>Mon, 18 Sep 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Assignment 2 (released 22 Aug, due 30 Aug)</title>
  <link>https://github.com/nipunbatra/pml2023/assignments/2.html</link>
  <description><![CDATA[ 



<section id="instructions" class="level2">
<h2 class="anchored" data-anchor-id="instructions">Instructions</h2>
<ul>
<li>Total marks: 8</li>
<li>Use torch for the assignment</li>
<li>For distributions use torch.distributions and do not use torch.random directly</li>
<li>The assignment has to be done in groups of two.</li>
<li>The assignment should be a single Jupyter notebook.</li>
<li>The results from every question of your assignment should be in visual formats such as plots, tables. Don’t show model’s log directly in Viva. All plots should have labels and legends appropriately. If not done, we may cut some marks for presentation (e.g.&nbsp;10%).</li>
<li>To know more about a distribution, just look at the Wikipedia page.</li>
</ul>
</section>
<section id="questions" class="level2">
<h2 class="anchored" data-anchor-id="questions">Questions</h2>
<ol type="1">
<li>[Total marks: 2.5] Consider the dataset below (1.1). Find MLE estimate for parameters of a neural network for regression with Gaussian Homoskedastic noise, where noise variance has a fixed value = 0.0025. Your model summary should match with (1.2). Animate the MLE fit on the data along with the 95% noise variance intervals [2 marks]. What is the effect of varying the noise variance (only in model, not for regenerating the data) on the MLE fit, show it for 3 different noise variance values? [0.5 mark] Refer to <a href="https://machinelearningmastery.com/develop-your-first-neural-network-with-pytorch-step-by-step/">this tutorial</a> for building and training <code>torch.nn</code> models. Use <a href="https://matplotlib.org/stable/tutorials/introductory/animation_tutorial.html">FuncAnimation</a> from matplotlib or <a href="https://github.com/jwkvam/celluloid">Celluloid</a> for animation.</li>
</ol>
<p>1.1 Data Generation</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-2"></span>
<span id="cb1-3">np.random.seed(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb1-4">X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linspace(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)</span>
<span id="cb1-5">noise <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.normal(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)</span>
<span id="cb1-6">y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.sin(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.pi <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> X) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> noise</span></code></pre></div>
<p>1.2 Model Summary</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb2-1">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ... <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Your model</span></span>
<span id="cb2-2"></span>
<span id="cb2-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torchsummary <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> summary</span>
<span id="cb2-4">summary(model, (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,))</span></code></pre></div>
<pre><code>----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                   [-1, 10]              20
              GELU-2                   [-1, 10]               0
            Linear-3                   [-1, 10]             110
              SELU-4                   [-1, 10]               0
            Linear-5                    [-1, 1]              11
================================================================
Total params: 141
Trainable params: 141
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.00
----------------------------------------------------------------</code></pre>
<ol start="2" type="1">
<li>[Total marks: 1.5] You toss a coin 10 times and the result turns out to be: <code>[1, 0, 0, 1, 1, 0, 1, 0, 0, 0]</code>. Find the MAP estimate for <code>probability of heads</code> if:
<ol type="i">
<li>Prior is Beta distribution with parameters (alpha=2, beta=3):
<ol type="a">
<li>Calculate the answer analytically using the closed form MAP estimate [0.5 mark]</li>
<li>Find the answer with gradient descent using <code>torch.optim</code> [0.5 mark]</li>
</ol></li>
<li>Prior is a Gaussian distribution with mean=0.5 and variance=0.1. Find the answer with gradient descent using <code>torch.optim</code> [0.5 mark]</li>
</ol></li>
<li>[Total marks: 2.5] Generate a linear trend dataset with the following code (3.1). Find MAP estimate for slope and intercept:
<ol type="i">
<li>Prior is Normal distribution with mean=0 and variance=1 [1 marks]</li>
<li>Show the effect of varying the size of the dataset on the MAP estimate [0.5 mark]</li>
<li>Show the effect of varying the prior variance on the MAP estimate [0.5 mark]</li>
<li>Change the prior to Laplace with mean = 0 and scale = 1. Compare the MAP estimate with the one obtained in (i). What do you observe? [0.5 mark]</li>
</ol></li>
</ol>
<p>3.1 Generate a linear trend dataset</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb4-2">slope <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span></span>
<span id="cb4-3">intercept <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb4-4">N <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span></span>
<span id="cb4-5">X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linspace(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, N)</span>
<span id="cb4-6">noise <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.normal(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>, N)</span>
<span id="cb4-7">y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> slope <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> intercept <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> noise</span></code></pre></div>
<ol start="4" type="1">
<li>[Total marks: 1.5] Generate a classification dataset with the following code (4.1). Find the MAP estimate for the parameters of logistic regression. Assume Normal prior with mean=0 and variance=0.1 for all parameters. Visualize the MLE and MAP classification boundaries. What do you observe? [1.5 marks]</li>
</ol>
<p>4.1 Generate a classification dataset</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> make_blobs</span>
<span id="cb5-2"></span>
<span id="cb5-3">X, y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> make_blobs(</span>
<span id="cb5-4">    n_samples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, centers<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, n_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, cluster_std<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span></span>
<span id="cb5-5">)</span></code></pre></div>


</section>

 ]]></description>
  <guid>https://github.com/nipunbatra/pml2023/assignments/2.html</guid>
  <pubDate>Tue, 22 Aug 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Assignment 1 (released 10 Aug, due 18 Aug)</title>
  <link>https://github.com/nipunbatra/pml2023/assignments/1.html</link>
  <description><![CDATA[ 



<section id="instructions" class="level2">
<h2 class="anchored" data-anchor-id="instructions">Instructions</h2>
<ul>
<li>Total marks: 6</li>
<li>Use torch for the assignment</li>
<li>For distributions use torch.distributions and do not use torch.random directly</li>
<li>The assignment has to be done in groups of two.</li>
<li>The assignment should be a single Jupyter notebook.</li>
<li>The results from every question of your assignment should be in visual formats such as plots, tables. Don’t show model’s log directly in Viva. All plots should have labels and legends appropriately. If not done, we may cut some marks for presentation (e.g.&nbsp;10%).</li>
<li>To know more about a distribution, just look at the Wikipedia page.</li>
</ul>
</section>
<section id="questions" class="level2">
<h2 class="anchored" data-anchor-id="questions">Questions</h2>
<ol type="1">
<li><p>Optimise the following function using torch autograd and gradient descent, f(θ) = (θ₀ - 2)² + (θ₁ - 3)². In addition to finding the optima, you need to show the convergence plots. [0.5 marks]</p></li>
<li><p>Generate some data (100 data points) using a univariate Normal distribution with <code>loc=2.0</code> and <code>scale=4.0</code>.</p>
<ol type="a">
<li><p>Plot a 2d contour plot showing the Likelihood or the Log-Likelihood as a function of <code>loc</code> and <code>scale</code>. Please label all the axes including the colorbar. [1 mark]</p></li>
<li><p>Find the MLE parameters for the <code>loc</code> and <code>scale</code> using gradient descent. Plot convergence plot as well. [1 mark]</p></li>
<li><p>Redo the above question but learn <code>log(scale)</code> instead of <code>scale</code> and then finally transform to learn <code>scale</code>. What can you conclude? Why is this transformation useful? [0.5 mark]</p></li>
</ol></li>
<li><p>Generate some data (1000 data points) using a univariate Normal distribution with <code>loc=2.0</code> and <code>scale=4.0</code> and using Student-T distributions with varying degrees (from 1-8) of freedom (1000 data points corresponding to each degree of freedom). Plot the pdf (and logpdf) at uniformly spaced data from (-50, 50) in steps of 0.1. What can you conclude? [1 mark]</p></li>
<li><p>Analytically derive the MLE for exponential distribution. Generate some data (1000 data points) using some fixed parameter values and see if you can recover the analytical parameters using gradient descent based solution for obtaining MLE. [1 mark]</p></li>
<li><p>Generate some data (100 data points) using a univariate Normal distribution with <code>loc=2.0</code> and <code>scale=4.0</code>. Now, create datasets of size 10, 20, 50, 100, 500, 1000, 5000, 10000. We will use a different random seed to create ten different datasets for each of these sizes. For each of these datasets, find the MLE parameters for the <code>loc</code> and <code>scale</code> using gradient descent. Plot the estimates of <code>loc</code> and <code>scale</code> as a function of the dataset size. What can you conclude? [1 mark]</p></li>
</ol>


</section>

 ]]></description>
  <guid>https://github.com/nipunbatra/pml2023/assignments/1.html</guid>
  <pubDate>Thu, 10 Aug 2023 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
