<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>ES661 PML</title>
<link>https://github.com/nipunbatra/pml2023/assignments.html</link>
<atom:link href="https://github.com/nipunbatra/pml2023/assignments.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.3.450</generator>
<lastBuildDate>Tue, 22 Aug 2023 00:00:00 GMT</lastBuildDate>
<item>
  <title>Assignment 2 (released 22 Aug, due 30 Aug)</title>
  <link>https://github.com/nipunbatra/pml2023/assignments/2.html</link>
  <description><![CDATA[ 



<section id="instructions" class="level2">
<h2 class="anchored" data-anchor-id="instructions">Instructions</h2>
<ul>
<li>Total marks: 8</li>
<li>Use torch for the assignment</li>
<li>For distributions use torch.distributions and do not use torch.random directly</li>
<li>The assignment has to be done in groups of two.</li>
<li>The assignment should be a single Jupyter notebook.</li>
<li>The results from every question of your assignment should be in visual formats such as plots, tables. Don’t show model’s log directly in Viva. All plots should have labels and legends appropriately. If not done, we may cut some marks for presentation (e.g.&nbsp;10%).</li>
<li>To know more about a distribution, just look at the Wikipedia page.</li>
</ul>
</section>
<section id="questions" class="level2">
<h2 class="anchored" data-anchor-id="questions">Questions</h2>
<ol type="1">
<li>[Total marks: 2.5] Consider the dataset below (1.1). Find MLE estimate for parameters of a neural network for regression with Gaussian Homoskedastic noise, where noise variance has a fixed value = 0.0025. Your model summary should match with (1.2). Animate the MLE fit on the data along with the 95% noise variance intervals [2 marks]. What is the effect of varying the noise variance (only in model, not for regenerating the data) on the MLE fit, show it for 3 different noise variance values? [0.5 mark] Refer to <a href="https://machinelearningmastery.com/develop-your-first-neural-network-with-pytorch-step-by-step/">this tutorial</a> for building and training <code>torch.nn</code> models. Use <a href="https://matplotlib.org/stable/tutorials/introductory/animation_tutorial.html">FuncAnimation</a> from matplotlib or <a href="https://github.com/jwkvam/celluloid">Celluloid</a> for animation.</li>
</ol>
<p>1.1 Data Generation</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-2"></span>
<span id="cb1-3">np.random.seed(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb1-4">X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linspace(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)</span>
<span id="cb1-5">noise <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.normal(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)</span>
<span id="cb1-6">y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.sin(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.pi <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> X) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> noise</span></code></pre></div>
<p>1.2 Model Summary</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb2-1">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ... <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Your model</span></span>
<span id="cb2-2"></span>
<span id="cb2-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torchsummary <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> summary</span>
<span id="cb2-4">summary(model, (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,))</span></code></pre></div>
<pre><code>----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                   [-1, 10]              20
              GELU-2                   [-1, 10]               0
            Linear-3                   [-1, 10]             110
              SELU-4                   [-1, 10]               0
            Linear-5                    [-1, 1]              11
================================================================
Total params: 141
Trainable params: 141
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.00
----------------------------------------------------------------</code></pre>
<ol start="2" type="1">
<li>[Total marks: 1.5] You toss a coin 10 times and the result turns out to be: <code>[1, 0, 0, 1, 1, 0, 1, 0, 0, 0]</code>. Find the MAP estimate for <code>probability of heads</code> if:
<ol type="i">
<li>Prior is Beta distribution with parameters (alpha=2, beta=3):
<ol type="a">
<li>Calculate the answer analytically using the closed form MAP estimate [0.5 mark]</li>
<li>Find the answer with gradient descent using <code>torch.optim</code> [0.5 mark]</li>
</ol></li>
<li>Prior is a Gaussian distribution with mean=0.5 and variance=0.1. Find the answer with gradient descent using <code>torch.optim</code> [0.5 mark]</li>
</ol></li>
<li>[Total marks: 2.5] Generate a linear trend dataset with the following code (3.1). Find MAP estimate for slope and intercept:
<ol type="i">
<li>Prior is Normal distribution with mean=0 and variance=1 [1 marks]</li>
<li>Show the effect of varying the size of the dataset on the MAP estimate [0.5 mark]</li>
<li>Show the effect of varying the prior variance on the MAP estimate [0.5 mark]</li>
<li>Change the prior to Laplace with mean = 0 and scale = 1. Compare the MAP estimate with the one obtained in (i). What do you observe? [0.5 mark]</li>
</ol></li>
</ol>
<p>3.1 Generate a linear trend dataset</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb4-2">slope <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span></span>
<span id="cb4-3">intercept <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb4-4">N <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span></span>
<span id="cb4-5">X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linspace(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, N)</span>
<span id="cb4-6">noise <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.normal(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>, N)</span>
<span id="cb4-7">y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> slope <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> intercept <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> noise</span></code></pre></div>
<ol start="4" type="1">
<li>[Total marks: 1.5] Generate a classification dataset with the following code (4.1). Find the MAP estimate for the parameters of logistic regression. Assume Normal prior with mean=0 and variance=0.1 for all parameters. Visualize the MLE and MAP classification boundaries. What do you observe? [1.5 marks]</li>
</ol>
<p>4.1 Generate a classification dataset</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> make_blobs</span>
<span id="cb5-2"></span>
<span id="cb5-3">X, y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> make_blobs(</span>
<span id="cb5-4">    n_samples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, centers<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, n_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, cluster_std<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span></span>
<span id="cb5-5">)</span></code></pre></div>


</section>

 ]]></description>
  <guid>https://github.com/nipunbatra/pml2023/assignments/2.html</guid>
  <pubDate>Tue, 22 Aug 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Assignment 2 (released 22 Aug, due 30 Aug)</title>
  <link>https://github.com/nipunbatra/pml2023/assignments/3.html</link>
  <description><![CDATA[ 



<section id="instructions" class="level2">
<h2 class="anchored" data-anchor-id="instructions">Instructions</h2>
<ul>
<li>Total marks: 8</li>
<li>Use torch for the assignment</li>
<li>For distributions use torch.distributions and do not use torch.random directly</li>
<li>The assignment has to be done in groups of two.</li>
<li>The assignment should be a single Jupyter notebook.</li>
<li>The results from every question of your assignment should be in visual formats such as plots, tables. Don’t show model’s log directly in Viva. All plots should have labels and legends appropriately. If not done, we may cut some marks for presentation (e.g.&nbsp;10%).</li>
<li>To know more about a distribution, just look at the Wikipedia page.</li>
</ul>
</section>
<section id="under-construction-dont-solve-till-this-note-is-removed" class="level2">
<h2 class="anchored" data-anchor-id="under-construction-dont-solve-till-this-note-is-removed">UNDER CONSTRUCTION (DON”T SOLVE TILL THIS NOTE IS REMOVED)</h2>
</section>
<section id="questions" class="level2">
<h2 class="anchored" data-anchor-id="questions">Questions</h2>
<ol type="1">
<li><p>Approximate the normalization constant for standard normal distribution using Monte Carlo integration. We only know the numerator term: e^ and want to find <img src="https://latex.codecogs.com/png.latex?I%20=%20%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7D%20e%5E%5Cfrac%7B-x%5E2%7D%7B2%7D%20dx"></p>
<ol type="i">
<li>What do you choose as f(x) and p(x)? [0.5 mark]</li>
<li>Estimate <code>I</code> using Monte Carlo integration for varying number of samples. For each number of samples, repeat the experiment 10 times and report the mean and standard deviation of the estimate. [1 mark]</li>
<li>scipy.integrate.quad</li>
</ol></li>
</ol>
<p>Use 1000 samples. [1 mark]</p>
<ol start="2" type="1">
<li><p>Inverse CDF</p></li>
<li><p>Rejection sampling</p></li>
<li><p>Laplace approximation (w/ change of variable?; BNN)</p></li>
<li><p>BMH data https://nbviewer.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter1_Introduction/Ch1_Introduction_PyMC3.ipynb</p></li>
</ol>


</section>

 ]]></description>
  <guid>https://github.com/nipunbatra/pml2023/assignments/3.html</guid>
  <pubDate>Tue, 22 Aug 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Assignment 1 (released 10 Aug, due 18 Aug)</title>
  <link>https://github.com/nipunbatra/pml2023/assignments/1.html</link>
  <description><![CDATA[ 



<section id="instructions" class="level2">
<h2 class="anchored" data-anchor-id="instructions">Instructions</h2>
<ul>
<li>Total marks: 6</li>
<li>Use torch for the assignment</li>
<li>For distributions use torch.distributions and do not use torch.random directly</li>
<li>The assignment has to be done in groups of two.</li>
<li>The assignment should be a single Jupyter notebook.</li>
<li>The results from every question of your assignment should be in visual formats such as plots, tables. Don’t show model’s log directly in Viva. All plots should have labels and legends appropriately. If not done, we may cut some marks for presentation (e.g.&nbsp;10%).</li>
<li>To know more about a distribution, just look at the Wikipedia page.</li>
</ul>
</section>
<section id="questions" class="level2">
<h2 class="anchored" data-anchor-id="questions">Questions</h2>
<ol type="1">
<li><p>Optimise the following function using torch autograd and gradient descent, f(θ) = (θ₀ - 2)² + (θ₁ - 3)². In addition to finding the optima, you need to show the convergence plots. [0.5 marks]</p></li>
<li><p>Generate some data (100 data points) using a univariate Normal distribution with <code>loc=2.0</code> and <code>scale=4.0</code>.</p>
<ol type="a">
<li><p>Plot a 2d contour plot showing the Likelihood or the Log-Likelihood as a function of <code>loc</code> and <code>scale</code>. Please label all the axes including the colorbar. [1 mark]</p></li>
<li><p>Find the MLE parameters for the <code>loc</code> and <code>scale</code> using gradient descent. Plot convergence plot as well. [1 mark]</p></li>
<li><p>Redo the above question but learn <code>log(scale)</code> instead of <code>scale</code> and then finally transform to learn <code>scale</code>. What can you conclude? Why is this transformation useful? [0.5 mark]</p></li>
</ol></li>
<li><p>Generate some data (1000 data points) using a univariate Normal distribution with <code>loc=2.0</code> and <code>scale=4.0</code> and using Student-T distributions with varying degrees (from 1-8) of freedom (1000 data points corresponding to each degree of freedom). Plot the pdf (and logpdf) at uniformly spaced data from (-50, 50) in steps of 0.1. What can you conclude? [1 mark]</p></li>
<li><p>Analytically derive the MLE for exponential distribution. Generate some data (1000 data points) using some fixed parameter values and see if you can recover the analytical parameters using gradient descent based solution for obtaining MLE. [1 mark]</p></li>
<li><p>Generate some data (100 data points) using a univariate Normal distribution with <code>loc=2.0</code> and <code>scale=4.0</code>. Now, create datasets of size 10, 20, 50, 100, 500, 1000, 5000, 10000. We will use a different random seed to create ten different datasets for each of these sizes. For each of these datasets, find the MLE parameters for the <code>loc</code> and <code>scale</code> using gradient descent. Plot the estimates of <code>loc</code> and <code>scale</code> as a function of the dataset size. What can you conclude? [1 mark]</p></li>
</ol>


</section>

 ]]></description>
  <guid>https://github.com/nipunbatra/pml2023/assignments/1.html</guid>
  <pubDate>Thu, 10 Aug 2023 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
