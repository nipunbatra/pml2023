[
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nOct 28, 2023\n\n\nAssignment 4 (released 28 Oct, due 8 Nov 7 PM)\n\n\n\n\n\n\n\nSep 18, 2023\n\n\nAssignment 3 (released 18 Sep, due 27 September 7 PM)\n\n\n\n\n\n\n\nAug 22, 2023\n\n\nAssignment 2 (released 22 Aug, due 30 Aug)\n\n\n\n\n\n\n\nAug 10, 2023\n\n\nAssignment 1 (released 10 Aug, due 18 Aug)\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The YouTube playlist is linked here. The page containing the notebooks is linked here.\n\n\n\nLecture #\nDate\nTopic\nReading\n\n\n\n\n1\nAug 3\nIntroduction and Logistics [Slides], [YouTube]\n\n\n\n2\nAug 7\nBayes Rule for ML [Slides], [YouTube], [Notebook]\n\n\n\n3\nAug 10\nDistributions, Sampling, Likelihood [Slides][Youtube] [Notebook on Distributions], [Notebook on likelihood], [Notebook on MLE]\n\n\n\n4\nAug 14\nMaximum Likelihood Estimation (Univariate Normal) [Slides] [Revision on bias/variance of estimators in context of ML prediction], [SGD is unbiased], [Notebook on likelihood], [Notebook on MLE], [Notebook on biased estimators], [Youtube]\n\n\n\n5\nAug 17\nMLE for linear and logistic regression [Slides], [Notebook], [Youtube]\n\n\n\n6\nAug 21\nMAP for coin toss, Multivariate Normal [Slides], [Notebook on MAP], [Youtube]\n\n\n\n7\nAug 24\nBayesian Linear Regression [Slides], [Notebook MVN], [Youtube]\n\n\n\n8\nAug 28\nLaplace Appoximation [Slides], [Notebook], Calculus [Slides], [Notebook] [Youtube]\n\n\n\n9\nAug 31\nMonte Carlo Sampling [Slides], [[Notebooks]], [Youtube]\n\n\n\n10\nSep 11\nMonte Carlo Sampling and Sampling from common distributions [Slides], [[Notebooks]], [YouTube]\n\n\n\n11\nSep 14\nNeural Networks uncertainty [Notebook], [YouTube]\n\n\n\n12\nSep 18\nRejection and Importance Sampling [Slides] [Notebook] [YouTube]\n\n\n\n13\nSep 21\nPractical MCMC [Notebook] [Youtube]\n\n\n\n14\nSep 25\nMCMC theory [Slides 1 on Markov Chain], [Slides on MCMC] [Markov Chain notebook]\n\n\n\n15\nOct 5\nCalibration [Youtube], [Notebook], [Tutorial on Calibration]\n\n\n\n16\nOct 9\nMC Dropout and Deep Ensemble [Youtube], [Notebook]\n\n\n\n17\nOct 16\nHierarchical Models, [Notebook], [YouTube]\n\n\n\n18\nOct 19\nMeta learning: Hypernets and Neural Processes, [Notebook], [YouTube]\n\n\n\n19\nOct 30\nMeta Learning: SIREN, Coordinate Network, Hypernets, Neural Processes [Notebook], [YouTube]\n\n\n\n20\nNov 2\nSelf-supervised learning [Notebook], [YouTube], [Survey article on SSLs]\n\n\n\n21\nNov 6\nActive Learning [Notebook], [Interactive article], [YouTube], [Survey Article], [Article on Deep Active Learning]\n\n\n\n22\nNov 9\nBayesian Optimisation [Notebook 1 on Motivation], [Notebook 2 on implementation from scratch], [YouTube], [Distill.Pub article], [Other reference]\n\n\n\n23\nNov 13\nKL divergence, Variational Inference, MC sampling, Reparameterisation [Notebook], [YouTube]\n\n\n\n24\nNov 16\nVariational Inference-II: ELBO, Coin Toss [Notebook][YouTube]\n\n\n\n25\nNov 20\nVariational Inference-III: Variational Autoencoders [Notebook], [YouTube]\n\n\n\n26\nNov 23\nConformal Prediction?/Neural Processes?/Gaussian Processes?"
  },
  {
    "objectID": "notebooks/biased-mle-normal.html",
    "href": "notebooks/biased-mle-normal.html",
    "title": "Biased and Unbiased Estimators",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n%matplotlib inline\n\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\ndist = torch.distributions.Normal(0, 1)\n\n# Generate data\ndata = dist.sample((100,))\n\n# Plot data\n_ = sns.displot(data, kde=True)\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/seaborn/axisgrid.py:88: UserWarning: This figure was using constrained_layout, but that is incompatible with subplots_adjust and/or tight_layout; disabling constrained_layout.\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\n\n# Population\nnorm = torch.distributions.Normal(0, 1)\nxs = torch.linspace(-10, 10, 1000)\nys = torch.exp(norm.log_prob(xs))\nplt.plot(xs, ys)\nplt.title('Population Distribution')\nplt.xlabel('x')\nplt.ylabel('p(x)')\nplt.savefig('../figures/mle/population-dist.pdf')\n\npopulation = norm.sample((100000,))\nplt.scatter(population, torch.zeros_like(population),  marker='|', alpha=0.1)\nplt.savefig('../figures/mle/population.pdf')\n\n\n\n\n\nplt.plot(xs, ys)\nsample_1 = population[torch.randperm(population.size(0))[:10]]\nsample_2 = population[torch.randperm(population.size(0))[:10]]\nplt.scatter(sample_1, torch.zeros_like(sample_1),label='sample 1')\nplt.scatter(sample_2, torch.zeros_like(sample_2),  label='sample 2')\nplt.legend()\nplt.savefig('../figures/mle/sample.pdf')\n\n\n\n\n\nnorm = torch.distributions.Normal(0, 1)\npopulation = norm.sample((100000,))\n\ndef plot_fit(seed, num_samples):\n    torch.manual_seed(seed)\n    # Select a random sample of size num_samples from the population\n    data = population[torch.randperm(population.shape[0])[:num_samples]]\n    mu = data.mean()\n    sigma_2 = data.var(correction=0)\n\n    # Plot data scatter\n    plt.scatter(data, torch.zeros_like(data), color='black', marker='x', zorder=10, label='samples')\n    # Plot true distribution\n    x = torch.linspace(-3, 3, 100)\n    plt.plot(x, norm.log_prob(x).exp(), color='black', label='true distribution')\n    # Plot estimated distribution\n    est = torch.distributions.Normal(mu, sigma_2.sqrt())\n    plt.plot(x, est.log_prob(x).exp(), color='red', label='estimated distribution')\n    plt.legend()\n    plt.title(f\"Sample size: {num_samples}\\n\" +fr\"Sample parameters: $\\hat{{\\mu}}={mu:0.02f}$, $\\hat{{\\sigma^2}}={sigma_2:0.02f}$\")\n    plt.ylim(-0.1, 1.2)\n    plt.xlabel(\"$x$\")\n    plt.ylabel(\"$p(x)$\")\n    plt.savefig(f\"../figures/mle/biased-mle-normal-{num_samples}-{seed}.pdf\", bbox_inches='tight')\n    return mu, sigma_2\n\n\nN_samples = 100\nmus = {}\nsigmas = {}\nfor draw in [3, 4, 5, 10, 100]:\n    mus[draw] = torch.zeros(N_samples)\n    sigmas[draw] = torch.zeros(N_samples)\n    for i in range(N_samples):\n        plt.clf()\n        mus[draw][i], sigmas[draw][i] = plot_fit(i, draw)\n\n\n\n\n\nfor draw in [3, 4, 5, 10, 100]:\n    plt.clf()\n    plt.scatter(mus[draw], sigmas[draw])\n    plt.axhline(y=1, color='k', linestyle='--', label=r'$\\sigma^2$')\n    plt.axvline(x=0, color='g', linestyle='-.', label=r'$\\mu$')\n    plt.xlabel(r'$\\hat{\\mu}$')\n    plt.ylabel(r'$\\hat{\\sigma^2}$')\n    plt.legend()\n    plt.title(f'Sample size={draw}\\n'+ fr'$E[\\hat{{\\mu}}] = {mus[draw].mean():0.2f}$ $E[\\hat{{\\sigma^2}}] = {sigmas[draw].mean():0.2f}$ ')\n    plt.savefig(f\"../figures/mle/biased-mle-normal-scatter-{draw}.pdf\", bbox_inches='tight')\n    #plt.clf()\n\n\n\n\n\ndf = pd.DataFrame({draw: \n              sigmas[draw].numpy() \n              for draw in [3, 4, 5, 10, 100]}).mean()\ndf.plot(kind='bar', rot=0)\nplt.axhline(1, color='k', linestyle='--')\n# Put numbers on top of bars\nfor i, v in enumerate(df):\n    plt.text(i - .1, v + .01, f'{v:.3f}', color='k', fontsize=12)\n\nplt.xlabel(\"Sample size (N)\")\nplt.ylabel(\"Estimated standard deviation\")\nplt.savefig('../figures/biased-mle-variance-quality.pdf', bbox_inches='tight')\n\n\n\n\n\n\n\n3      0.981293\n4      1.014205\n5      0.952345\n10     1.022295\n100    0.985779\ndtype: float64\n\n\n\ndf_unbiased = df*(df.index/(df.index-1.0))\n\ndf_unbiased.plot(kind='bar', rot=0)\nplt.axhline(1, color='k', linestyle='--')\n# Put numbers on top of bars\nfor i, v in enumerate(df_unbiased):\n    plt.text(i - .1, v + .01, f'{v:.3f}', color='k', fontsize=12)\n\nplt.xlabel(\"Sample size (N)\")\nplt.ylabel(\"Corrected standard deviation\")\nplt.savefig('../figures/corrected-mle-variance-quality.pdf', bbox_inches='tight')"
  },
  {
    "objectID": "notebooks/distributions.html",
    "href": "notebooks/distributions.html",
    "title": "Discrete distributions",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\nBernoulli distribution\nThe PDF of the Bernoulli distribution is given by\n\\[\nf(x) = p^x (1-p)^{1-x}\n\\]\nwhere \\(x \\in \\{0, 1\\}\\) and \\(p \\in [0, 1]\\).\n\nbernoulli = torch.distributions.Bernoulli(probs=0.3)\nbernoulli.probs\n\ntensor(0.3000)\n\n\n\n# Plot PDF\np_0 = bernoulli.probs.item()\np_1 = 1 - p_0\n\nplt.bar([0, 1], [p_0, p_1], color='C0', edgecolor='k')\nplt.ylim(0, 1)\nplt.xticks([0, 1], ['0', '1'])\n\n([&lt;matplotlib.axis.XTick at 0x7f4c5e1f1790&gt;,\n  &lt;matplotlib.axis.XTick at 0x7f4c5e1f1760&gt;],\n [Text(0, 0, '0'), Text(1, 0, '1')])\n\n\n\n\n\n\n### Careful!\nbernoulli = torch.distributions.Bernoulli(logits=0.3)\nbernoulli.probs\n\ntensor(0.5744)\n\n\nLogits?!\nProbs range from 0 to 1, logits range from -inf to inf. Logits are the inverse of the sigmoid function.\nThe sigmoid function is defined as:\n\\[\\sigma(x) = \\frac{1}{1 + e^{-x}}\\]\nThe inverse of the sigmoid function is defined as:\n\\[\\sigma^{-1}(x) = \\log \\frac{x}{1 - x}\\]\n\n### Sampling\nbernoulli.sample()\n\ntensor(1.)\n\n\n\nbernoulli.sample((10,))\n\ntensor([0., 1., 0., 0., 1., 1., 0., 1., 0., 0.])\n\n\n\ndata = bernoulli.sample((1000,))\ndata\n\ntensor([1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n        0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0.,\n        0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1.,\n        1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.,\n        1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1.,\n        0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n        1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0.,\n        1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n        1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,\n        0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0.,\n        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n        1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0.,\n        0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n        1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n        0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n        1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n        1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0.,\n        1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,\n        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1.,\n        1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0.,\n        1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n        1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1.,\n        0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n        1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1.,\n        0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n        1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n        0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n        1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,\n        0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,\n        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1.,\n        1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0.,\n        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n        1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0.,\n        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n        1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0.,\n        1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,\n        1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1.,\n        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1.,\n        1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n        0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,\n        1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1.,\n        1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n        0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0.,\n        0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n        0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,\n        1., 1., 0., 0., 1., 1., 1., 0., 0., 1.])\n\n\n\n### Count number of 1s\ndata.sum()\n\ntensor(586.)\n\n\n\n### IID sampling\nsize = 1000\ndata = torch.empty(size)\nfor s_num in range(size):\n    dist = torch.distributions.Bernoulli(probs=0.3) # Each sample uses the same distribution (Identical)\n    data[s_num] = dist.sample() # Each sample is independent (Independent)\n\n\n### Dependent sampling\nsize = 1000\n\n### If previous sample was 1, next sample is 1 with probability 0.9\n### If previous sample was 1, next sample is 0 with probability 0.1\n### If previous sample was 0, next sample is 0 with probability 0.8\n### If previous sample was 0, next sample is 1 with probability 0.2\n\n\n### Categorical distribution\n\np1 = 0.2\np2 = 0.3\np3 = 0.5\n\ncategorical = torch.distributions.Categorical(probs=torch.tensor([p1, p2, p3]))\ncategorical.probs\n\ntensor([0.2000, 0.3000, 0.5000])\n\n\n\n# Plot PDF\n\nplt.bar([0, 1, 2], [p1, p2, p3], color='C0', edgecolor='k')\nplt.ylim(0, 1)\nplt.xticks([0, 1, 2], ['0', '1', '2'])\n\n([&lt;matplotlib.axis.XTick at 0x7f4c5c094ac0&gt;,\n  &lt;matplotlib.axis.XTick at 0x7f4c5c094a90&gt;,\n  &lt;matplotlib.axis.XTick at 0x7f4c5c094970&gt;],\n [Text(0, 0, '0'), Text(1, 0, '1'), Text(2, 0, '2')])\n\n\n\n\n\n\n### Uniform distribution\n\nuniform = torch.distributions.Uniform(low=0, high=1)\n\n\nuniform.sample()\n\ntensor(0.0553)\n\n\n\nuniform.support\n\nInterval(lower_bound=0.0, upper_bound=1.0)\n\n\n\n### Plot PDF\nxs = torch.linspace(0.0, 0.99999, 500)\nys = uniform.log_prob(xs).exp()\n\nplt.plot(xs, ys, color='C0')\n# Filled area\nplt.fill_between(xs, ys, color='C0', alpha=0.2)\n\n&lt;matplotlib.collections.PolyCollection at 0x7f4c5c01a940&gt;\n\n\n\n\n\n\n### Why log_prob? and not prob?\n\n\n### Normal distribution\n\nnormal = torch.distributions.Normal(loc=0, scale=1)\n\n\nnormal.support\n\nReal()\n\n\n\n### Plot PDF\nxs = torch.linspace(-5, 5, 500)\nys = normal.log_prob(xs).exp()\nplt.plot(xs, ys, color='C0')\n# Filled area\nplt.fill_between(xs, ys, color='C0', alpha=0.2)\n\n&lt;matplotlib.collections.PolyCollection at 0x7fe120a2f4c0&gt;\n\n\n\n\n\n\nxs = torch.linspace(-50, 50, 500)\nprobs = normal.log_prob(xs).exp()\nplt.plot(xs, probs, color='C0')\n# Filled area\n#plt.fill_between(xs, probs, color='C0', alpha=0.2)\n\n\n\n\n\nnormal.log_prob(torch.tensor(-20)), normal.log_prob(torch.tensor(-40))\n\nnormal.log_prob(torch.tensor(-20)).exp(), normal.log_prob(torch.tensor(-40)).exp()\n\n(tensor(0.), tensor(0.))\n\n\n\nxs = torch.linspace(-50, 50, 500)\nlogprobs = normal.log_prob(xs)\nplt.plot(xs, logprobs, color='C0')\n\n\n\n\n\ndef plot_normal(mu, sigma):\n    mu = torch.tensor(mu)\n    sigma = torch.tensor(sigma)\n    xs = torch.linspace(-40, 40, 1000)\n    dist = torch.distributions.Normal(mu, sigma)\n    \n    logprobs = dist.log_prob(xs)\n    probs = torch.exp(logprobs)\n    fig, ax = plt.subplots(nrows=2)\n    ax[0].plot(xs, probs)\n    ax[0].set_title(\"Probability\")\n    ax[1].plot(xs, logprobs)\n    ax[1].set_title(\"Log Probability\")\n\n\nplot_normal(0, 1)\n\n\n\n\n\n# Interactive slider for plot_normal function\n\nfrom ipywidgets import interact, FloatSlider\n\ninteract(plot_normal, mu=FloatSlider(min=-2, max=2, step=0.1, value=0), sigma=FloatSlider(min=0.1, max=2, step=0.1, value=1))\n\n\n\n\n&lt;function __main__.plot_normal(mu, sigma)&gt;\n\n\n\nsamples = normal.sample((1000,))\nsamples[:20]\n\ntensor([ 1.8764,  0.4868, -0.7966, -0.8190,  1.4538,  0.0766, -2.0262,  0.9965,\n        -1.1971, -0.4764, -2.1042,  0.2489, -0.2859,  1.1970, -0.7265, -0.8898,\n        -0.4592, -0.3581, -0.7239, -0.0790])\n\n\n\n_ = plt.hist(samples.numpy(), bins=50, density=True, edgecolor='k')\n\n\n\n\n\n_ = plt.hist(samples.numpy(), bins=30, density=True, edgecolor='k')\n\n\n\n\n\nimport seaborn as sns\nsns.kdeplot(samples.numpy(), bw_adjust=2.1, shade=True)\n\n&lt;AxesSubplot:ylabel='Density'&gt;\n\n\n\n\n\n\n### IID sampling\n\nn_samples = 1000\nsamples = []\nfor i in range(n_samples):\n    dist = torch.distributions.Normal(0, 1) # Using identical distribution over all samples\n    samples.append(dist.sample()) # sample is independent of previous samples\n\nsamples = torch.stack(samples)\n\nfig, ax = plt.subplots(nrows=2)\nsns.kdeplot(samples.numpy(), bw_adjust=2.0, shade=True, ax=ax[0])\nax[0].set_title(\"KDE of IID samples\")\n\nax[1].plot(samples.numpy())\nax[1].set_title(\"IID samples\")\n\nText(0.5, 1.0, 'IID samples')\n\n\n\n\n\n\n### Non-IID sampling (non-identical distribution)\n\nn_samples = 100\nsamples = []\nfor i in range(n_samples):\n    # Non-indentical distribution\n    if i%2:\n        dist = torch.distributions.Normal(torch.tensor([2.0]), torch.tensor([0.5]))\n    else:\n        dist = torch.distributions.Normal(torch.tensor([-2.0]), torch.tensor([0.5]))\n    samples.append(dist.sample())\n\nsamples = torch.stack(samples)\n\nfig, ax = plt.subplots(nrows=2)\nsns.kdeplot(samples.numpy().flatten(), bw_adjust=1.0, shade=True, ax=ax[0])\nax[0].set_title(\"KDE of non-identical samples\")\n\nax[1].plot(samples.numpy().flatten())\nax[1].set_title(\"Samples over time\")\n\nText(0.5, 1.0, 'Samples over time')\n\n\n\n\n\n\n### Non-IID sampling (dependent sampling)\n\nn_samples = 1000\nprev_sample = torch.tensor([10.0])\nsamples = []\nfor i in range(n_samples):\n    dist = torch.distributions.Normal(prev_sample, 1)\n    sample = dist.sample()\n    samples.append(sample)\n    prev_sample = sample\n\nsamples = torch.stack(samples)\nfig, ax = plt.subplots(nrows=2)\nsns.kdeplot(samples.numpy().flatten(), bw_adjust=2.0, shade=True, ax=ax[0])\nax[0].set_title(\"KDE of samples\")\n\nax[1].plot(samples.numpy().flatten())\nax[1].set_title(\"IID samples\")    \n\nText(0.5, 1.0, 'IID samples')\n\n\n\n\n\n\n### Laplace distribution v/s Normal distribution\n\nlaplace = torch.distributions.Laplace(loc=0, scale=1)\nnormal = torch.distributions.Normal(loc=0, scale=1)\nstudent_t_1 = torch.distributions.StudentT(df=1)\nstudent_t_2 = torch.distributions.StudentT(df=2)\n\n\nxs = torch.linspace(-6, 6, 500)\nys_laplace = laplace.log_prob(xs).exp()\nplt.plot(xs, ys_laplace, color='C0', label='Laplace')\n\nys_normal = normal.log_prob(xs).exp()\nplt.plot(xs, ys_normal, color='C1', label='Normal')\n\nys_student_t_1 = student_t_1.log_prob(xs).exp()\nplt.plot(xs, ys_student_t_1, color='C2', label='Student T (df=1)')\n\nys_student_t_2 = student_t_2.log_prob(xs).exp()\nplt.plot(xs, ys_student_t_2, color='C3', label='Student T (df=2)')\n\nplt.legend()\n\nzoom  = False\nif zoom:\n    plt.xlim(5, 6)\n    plt.ylim(-0.002, 0.02)\n\n\n\n\n\n### Beta distribution\n\nbeta = torch.distributions.Beta(concentration1=2, concentration0=2)\nbeta.support\n\nInterval(lower_bound=0.0, upper_bound=1.0)\n\n\n\n# PDF\nxs = torch.linspace(0, 1, 500)\nys = beta.log_prob(xs).exp()\nplt.plot(xs, ys, color='C0')\n# Filled area\nplt.fill_between(xs, ys, color='C0', alpha=0.2)\n\n&lt;matplotlib.collections.PolyCollection at 0x7f5125cf0430&gt;\n\n\n\n\n\n\ns = beta.sample()\ns\n\ntensor(0.3356)\n\n\n\n# Add widget to play with parameters\nfrom ipywidgets import interact\n\ndef plot_beta(a, b):\n    beta = torch.distributions.Beta(concentration1=a, concentration0=b)\n    xs = torch.linspace(0, 1, 500)\n    ys = beta.log_prob(xs).exp()\n    plt.plot(xs, ys, color='C0')\n    # Filled area\n    plt.fill_between(xs, ys, color='C0', alpha=0.2)\n\ninteract(plot_beta,a=(0.1, 10, 0.1), b=(0.1, 10, 0.1))\n\n\n\n\n&lt;function __main__.plot_beta(a, b)&gt;\n\n\n\n### Dirichlet distribution\n\ndirichlet = torch.distributions.Dirichlet(concentration=torch.tensor([2.0, 2.0, 2.0]))\ndirichlet.support\n\nSimplex()\n\n\n\ns = dirichlet.sample()\nprint(s, s.sum())\n\ntensor([0.2924, 0.3254, 0.3821]) tensor(1.)\n\n\n\ns = dirichlet.sample()\nprint(s, s.sum())\n\ntensor([0.3898, 0.1071, 0.5030]) tensor(1.)\n\n\n\ndirichlet2 = torch.distributions.Dirichlet(concentration=torch.tensor([0.8, 0.1, 0.1]))\ns = dirichlet2.sample()\nprint(s, s.sum())\n\ntensor([0.8190, 0.0111, 0.1699]) tensor(1.)"
  },
  {
    "objectID": "notebooks/mle-univariate.html",
    "href": "notebooks/mle-univariate.html",
    "title": "ES661 PML",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\nc1 = torch.distributions.Normal(80, 10)\nc2 = torch.distributions.Normal(70, 10)\nc3 = torch.distributions.Normal(90, 5)\n\n\n# Plot the distributions\nx = torch.linspace(0, 105, 1000)\nplt.plot(x, c1.log_prob(x).exp(), label='C1')\nplt.plot(x, c2.log_prob(x).exp(), label='C2')\nplt.plot(x, c3.log_prob(x).exp(), label='C3')\n# Fill the area under the curve\nplt.fill_between(x, c1.log_prob(x).exp(), alpha=0.2)\nplt.fill_between(x, c2.log_prob(x).exp(), alpha=0.2)\nplt.fill_between(x, c3.log_prob(x).exp(), alpha=0.2)\n\nplt.xlabel('Marks')\nplt.ylabel('Probability')\n\nplt.legend(title='Class')\nplt.savefig('../figures/mle/mle-example.pdf', bbox_inches='tight')\n\n# Vertical line at x = 85\nmarks = torch.tensor([82.])\nplt.axvline(marks.item(), color='k', linestyle='--', lw=2)\n# Draw horizontal line to show the probability at x = 85\nplt.hlines(c1.log_prob(marks).exp(), 0, marks.item(), color='C0', linestyle='--', lw=2)\nplt.hlines(c2.log_prob(marks).exp(), 0, marks.item(), color='C1', linestyle='--', lw=2)\nplt.hlines(c3.log_prob(marks).exp(), 0, marks.item(), color='C2', linestyle='--', lw=2)\nplt.savefig('../figures/mle/mle-example-2.pdf', bbox_inches='tight')\n\n\n\n\n\nobs = torch.tensor([20.0])\nsigma = torch.tensor([1.0])\n\n# Plot the likelihood\nmus = torch.linspace(0, 40, 1000)\nplt.plot(mus, torch.distributions.Normal(mus, sigma).log_prob(obs).exp())\nplt.xlabel(r'Parameter ($\\mu$)')\nplt.ylabel(r'Likelihood $p(x = 20|\\mu$)')\n\nText(0, 0.5, 'Likelihood $p(x = 20|\\\\mu$)')\n\n\nfindfont: Font family ['cursive'] not found. Falling back to DejaVu Sans.\nfindfont: Generic family 'cursive' not found because none of the following families were found: Apple Chancery, Textile, Zapf Chancery, Sand, Script MT, Felipa, Comic Neue, Comic Sans MS, cursive\n\n\n\n\n\n\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\n\n\n# Interactive plot showing fitting normal distribution of varying mu to one data point\n\ndef plot_norm(mu):\n    mu = torch.tensor(mu)\n    sigma = torch.tensor(1.0)\n    x = torch.tensor(20.0)\n    n = torch.distributions.Normal(mu, sigma)\n    x_lin = torch.linspace(0, 40, 500)\n    y_lin = n.log_prob(x_lin).exp()\n    likelihood = n.log_prob(x).exp()\n    plt.plot(x_lin, y_lin, label=rf\"$\\mathcal{{N}}({mu.item():0.4f}, 1)$\")\n    plt.legend()\n    plt.title(f\"Likelihood={likelihood:.4f}\")\n    plt.ylim(0, 0.5)\n    plt.fill_between(x_lin, y_lin, alpha=0.2)\n    plt.axvline(x=x, color=\"black\", linestyle=\"--\")\n    plt.axhline(y=likelihood, color=\"black\", linestyle=\"--\")\n        \n#plot_norm(20)\ninteract(plot_norm, mu=(0, 30, 0.1))\n\n\n\n\n&lt;function __main__.plot_norm(mu)&gt;\n\n\n\n# Interactive plot showing fitting normal distribution of varying mu to one data point\n\ndef plot_norm_log(mu):\n    mu = torch.tensor(mu)\n    sigma = torch.tensor(1.0)\n    x = torch.tensor(20.0)\n    n = torch.distributions.Normal(mu, sigma)\n    x_lin = torch.linspace(0, 40, 500)\n    fig, ax = plt.subplots(nrows=2, sharex=True)\n    y_log_lin = n.log_prob(x_lin)\n    y_lin = y_log_lin.exp()\n    ll = n.log_prob(x)\n    likelihood = ll.exp()\n    ax[0].plot(x_lin, y_lin, label=rf\"$\\mathcal{{N}}({mu.item():0.4f}, 1)$\")\n    #plt.legend()\n    ax[0].set_title(f\"Likelihood={likelihood:.4f}\")\n    ax[0].set_ylim(0, 0.5)\n    ax[0].fill_between(x_lin, y_lin, alpha=0.2)\n\n    ax[1].plot(x_lin, y_log_lin, label=rf\"$\\mathcal{{N}}({mu.item():0.4f}, 1)$\")\n    ax[1].set_title(f\"Log Likelihood={ll:.4f}\")\n    ax[1].set_ylim(-500, 20)\n    \n    ax[0].axvline(x=x, color=\"black\", linestyle=\"--\")\n    ax[0].axhline(y=likelihood, color=\"black\", linestyle=\"--\")\n\n    ax[1].axvline(x=x, color=\"black\", linestyle=\"--\")\n    ax[1].axhline(y=ll, color=\"black\", linestyle=\"--\")\n        \n#plot_norm_log(10)\ninteract(plot_norm_log, mu=(0, 30, 0.1))\n\n\n\n\n&lt;function __main__.plot_norm_log(mu)&gt;\n\n\n\n# Plot the distributions\ndef plot_class(class_num):\n    x = torch.linspace(0, 105, 1000)\n    dist = [c1, c2, c3][class_num-1]\n    plt.plot(x, dist.log_prob(x).exp(), label=f'C{class_num}')\n    plt.fill_between(x, dist.log_prob(x).exp(), alpha=0.2)\n\n\n    plt.xlabel('Marks')\n    plt.ylabel('Probability')\n\n    #plt.legend(title='Class')\n    #plt.savefig('../figures/mle/mle-example.pdf', bbox_inches='tight')\n\n    # Vertical line at x = 82\n    marks = torch.tensor([82., 72.0])\n    for mark in marks:\n        plt.axvline(mark.item(), color='k', linestyle='--', lw=2)\n\n        plt.hlines(dist.log_prob(mark).exp(), 0, mark.item(), color='C0', linestyle='--', lw=2, label=f\"P({mark.item()}|Class ={dist.log_prob(mark).exp().item():0.4f}\")\n        #plt.hlines(c2.log_prob(mark).exp(), 0, mark.item(), color='C1', linestyle='--', lw=2)\n        #plt.hlines(c3.log_prob(mark).exp(), 0, mark.item(), color='C2', linestyle='--', lw=2)\n    #plt.savefig('../figures/mle/mle-example-2.pdf', bbox_inches='tight')\n    plt.legend()\n    #plt.savefig(\"..\")\n\nplot_class(1)\n\n\n\n\n\nplot_class(2)\n\n\n\n\n\nplot_class(3)\n\n\n\n\n\n#\ns1 = torch.tensor([82.0])\ns2 = torch.tensor([72.0])\n\np_s1_c1 = c1.log_prob(s1).exp()\np_s1_c2 = c2.log_prob(s1).exp()\np_s1_c3 = c3.log_prob(s1).exp()\n\np_s2_c1 = c1.log_prob(s2).exp()\np_s2_c2 = c2.log_prob(s2).exp()\np_s2_c3 = c3.log_prob(s2).exp()\n\n# Create dataframe\ndf = pd.DataFrame({\n    'Class': ['C1', 'C2', 'C3'],\n    'Student 1 (82)': [p_s1_c1.item(), p_s1_c2.item(), p_s1_c3.item()],\n    'Student 2 (72)': [p_s2_c1.item(), p_s2_c2.item(), p_s2_c3.item()]\n})\n\ndf = df.set_index('Class')\ndf\n\n\n\n\n\n\n\n\nStudent 1 (82)\nStudent 2 (72)\n\n\nClass\n\n\n\n\n\n\nC1\n0.039104\n0.028969\n\n\nC2\n0.019419\n0.039104\n\n\nC3\n0.022184\n0.000122\n\n\n\n\n\n\n\n\ndf.plot(kind='bar', rot=0)\n\n&lt;AxesSubplot:xlabel='Class'&gt;\n\n\n\n\n\n\n# Multiply the probabilities\ndf.aggregate('prod', axis=1)\n\nClass\nC1    0.001133\nC2    0.000759\nC3    0.000003\ndtype: float64\n\n\n\ndf.aggregate('prod', axis=1).plot(kind='bar', rot=0)\n\n&lt;AxesSubplot:xlabel='Class'&gt;\n\n\n\n\n\n\n# Create a slider to change s1 and s2 marks and plot the likelihood\n\n\ndef plot_likelihood(s1, s2, scale='log'):\n    s1 = torch.tensor([s1])\n    s2 = torch.tensor([s2])\n    p_s1_c1 = c1.log_prob(s1)\n    p_s1_c2 = c2.log_prob(s1)\n    p_s1_c3 = c3.log_prob(s1)\n\n    p_s2_c1 = c1.log_prob(s2)\n    p_s2_c2 = c2.log_prob(s2)\n    p_s2_c3 = c3.log_prob(s2)\n\n\n    # Create dataframe\n    df = pd.DataFrame({\n        'Class': ['C1', 'C2', 'C3'],\n        f'Student 1 ({s1.item()})': [p_s1_c1.item(), p_s1_c2.item(), p_s1_c3.item()],\n        f'Student 2 ({s2.item()})': [p_s2_c1.item(), p_s2_c2.item(), p_s2_c3.item()]\n    })\n    \n    \n\n    df = df.set_index('Class')\n    if scale!='log':\n        df  = df.apply(np.exp)\n    df.plot(kind='bar', rot=0)\n    plt.ylabel('Probability')\n    plt.xlabel('Class')\n\n    if scale=='log':\n        plt.ylabel('Log Probability')\n        #plt.yscale('log')\n\nplot_likelihood(80, 72, scale='linear')\n\n    \n\n\n\n\n\nplot_likelihood(80, 72, scale='log')\n\n\n\n\n\n# Interactive plot\ninteract(plot_likelihood, s1=(0, 100), s2=(0, 100), scale=['linear', 'log'])\n\n\n\n\n&lt;function __main__.plot_likelihood(s1, s2, scale='log')&gt;\n\n\n\n# Let us now consider some N points from a univariate Gaussian distribution with mean 0 and variance 1.\n\nN = 5\ntorch.manual_seed(2)\nsamples = torch.distributions.Normal(0, 1).sample((N,))\nsamples\n\ntensor([ 0.3923, -0.2236, -0.3195, -1.2050,  1.0445])\n\n\n\nplt.scatter(samples, np.zeros_like(samples))\n\n&lt;matplotlib.collections.PathCollection at 0x7f5e96e106a0&gt;\n\n\n\n\n\n\ndef ll(mu, sigma):\n    mu = torch.tensor(mu)\n    sigma = torch.tensor(sigma)\n\n    dist = torch.distributions.Normal(mu, sigma)\n    loglik = dist.log_prob(samples).sum()\n    return dist, loglik\n\ndef plot_normal(mu, sigma):\n    xs = torch.linspace(-5, 5, 100)\n    dist, loglik = ll(mu, sigma)\n    ys_log = dist.log_prob(xs)\n    plt.plot(xs, ys_log)\n\n    plt.scatter(samples, dist.log_prob(samples), color='C3', alpha=0.5)\n    plt.title(f'log likelihood: {loglik:.8f}')\n   \n\n\nplot_normal(0, 2)\n#plt.ylim(-1.7, -1.6)\n\n\n\n\n\ninteract(plot_normal, mu=(-3.0, 3.0), sigma=(0.1, 10))\n\n\n\n\n&lt;function __main__.plot_normal(mu, sigma)&gt;\n\n\n\ndef get_lls(mus, sigmas):\n\n    lls = torch.zeros((len(mus), len(sigmas)))\n    for i, mu in enumerate(mus):\n        for j, sigma in enumerate(sigmas):\n\n            lls[i, j] = ll(mu, sigma)[1]\n    return lls\n\nmus = torch.linspace(-1, 1, 100)\nsigmas = torch.linspace(0.1, 1.5, 100)\nlls = get_lls(mus, sigmas)\n\n/tmp/ipykernel_302139/3787141935.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  mu = torch.tensor(mu)\n/tmp/ipykernel_302139/3787141935.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  sigma = torch.tensor(sigma)\n\n\n\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\ndef plot_lls(mus, sigmas, lls):\n    fig, ax1 = plt.subplots(figsize=(8, 6))\n    \n    X, Y = np.meshgrid(mus, sigmas)\n    \n    max_indices = np.unravel_index(np.argmax(lls), lls.shape)\n    max_mu = mus[max_indices[1]]\n    max_sigma = sigmas[max_indices[0]]\n    max_loglik = lls[max_indices]\n\n    # Define levels with increasing granularity\n    levels_low = np.linspace(lls.min(), max_loglik, 20)\n    levels_high = np.linspace(max_loglik + 0.001, lls.max(), 10)  # Adding a small value to prevent duplicates\n    levels = levels_low\n    \n    # Plot the contour filled plot\n    contour = ax1.contourf(X, Y, lls.T, levels=levels, cmap='magma')\n    \n    # Plot the contour lines\n    contour_lines = ax1.contour(X, Y, lls.T, levels=levels, colors='black', linewidths=0.5, alpha=0.6)\n    \n    # Add contour labels\n    ax1.clabel(contour_lines, inline=True, fontsize=10, colors='black', fmt='%1.2f')\n    \n    ax1.set_xlabel('Mu')\n    ax1.set_ylabel('Sigma')\n    ax1.set_title('Contour Plot of Log Likelihood')\n    \n    # Add maximum log likelihood point as scatter on the contour plot\n    ax1.scatter([max_mu], [max_sigma], color='red', marker='o', label='Maximum Log Likelihood')\n    ax1.annotate(f'Max LL: {max_loglik:.2f}', (max_mu, max_sigma), textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=10, color='red', bbox=dict(facecolor='white', alpha=0.8, edgecolor='none', boxstyle='round,pad=0.3'))\n\n    ax1.axvline(max_mu, color='red', linestyle='--', alpha=0.5)\n    ax1.axhline(max_sigma, color='red', linestyle='--', alpha=0.5)\n    \n    # Create colorbar outside the plot\n    divider = make_axes_locatable(ax1)\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n    cbar = plt.colorbar(contour, cax=cax)\n    cbar.set_label('Log Likelihood', rotation=270, labelpad=15)\n    \n    plt.tight_layout()\n    plt.show()\n\nplot_lls(mus, sigmas, lls)\n\n/tmp/ipykernel_302139/1128417763.py:44: UserWarning: This figure was using constrained_layout, but that is incompatible with subplots_adjust and/or tight_layout; disabling constrained_layout.\n  plt.tight_layout()\n\n\n\n\n\n\nsamples.mean(), samples.std(correction=0)\n\n(tensor(-0.0623), tensor(0.7523))\n\n\n\nmus = torch.linspace(-0.4, 0.4, 200)\nsigmas = torch.linspace(0.5, 1.0,200)\nlls = get_lls(mus, sigmas)\nplot_lls(mus, sigmas, lls)\n\n/tmp/ipykernel_302139/3787141935.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  mu = torch.tensor(mu)\n/tmp/ipykernel_302139/3787141935.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  sigma = torch.tensor(sigma)\n/tmp/ipykernel_302139/1128417763.py:44: UserWarning: This figure was using constrained_layout, but that is incompatible with subplots_adjust and/or tight_layout; disabling constrained_layout.\n  plt.tight_layout()"
  },
  {
    "objectID": "assignments/3.html",
    "href": "assignments/3.html",
    "title": "Assignment 3 (released 18 Sep, due 27 September 7 PM)",
    "section": "",
    "text": "Total marks: 9\nUse torch for the assignment\nFor sampling, use torch.distributions and do not use torch.random directly\nFor computing log_prob, use torch.distributions and do not use custom formulas\nThe assignment has to be done in groups of two.\nThe assignment should be a single Jupyter notebook.\nThe results from every question of your assignment should be in visual formats such as plots and tables. Don’t show model’s log directly in Viva. All plots should have labels and legends appropriately. If not done, we may cut some marks for presentation (e.g. 10%).\nTo know more about a distribution, just look at the Wikipedia page (table on the right will have everything you need)."
  },
  {
    "objectID": "assignments/3.html#instructions",
    "href": "assignments/3.html#instructions",
    "title": "Assignment 3 (released 18 Sep, due 27 September 7 PM)",
    "section": "",
    "text": "Total marks: 9\nUse torch for the assignment\nFor sampling, use torch.distributions and do not use torch.random directly\nFor computing log_prob, use torch.distributions and do not use custom formulas\nThe assignment has to be done in groups of two.\nThe assignment should be a single Jupyter notebook.\nThe results from every question of your assignment should be in visual formats such as plots and tables. Don’t show model’s log directly in Viva. All plots should have labels and legends appropriately. If not done, we may cut some marks for presentation (e.g. 10%).\nTo know more about a distribution, just look at the Wikipedia page (table on the right will have everything you need)."
  },
  {
    "objectID": "assignments/3.html#questions",
    "href": "assignments/3.html#questions",
    "title": "Assignment 3 (released 18 Sep, due 27 September 7 PM)",
    "section": "Questions",
    "text": "Questions\n\n[2.5 marks] Approximate the normalization constant for standard normal distribution using Monte Carlo integration. Assume that we only know the numerator term: \\(e^\\frac{-x^2}{2}\\) and want to find \\(I = \\int_{-\\infty}^{\\infty} e^\\frac{-x^2}{2} dx\\): \\[\\begin{equation}\n\\int f(x) p(x) dx \\approx \\frac{1}{N} \\sum_{i=1}^{N} f(x_i) \\text{ where } x_i \\sim p(x).\n\\end{equation}\\]\n\nTake p(x) = Uniform(-a, a). Choose the following values of a {0.01, 0.02, 0.05, 0.1, 0.5, 1, 2, 3, 5}. Draw 1000 samples for each a and plot the normalizing constant on y-axis for each value of a on x-axis. Draw a horizontal line showing the analytical normalizing constant as ground truth. [1 marks]\nEstimate I using Monte Carlo integration for varying number of MC samples {10, 100, 10^3 , 10^4, 10^5} for a=4. For each value of number of MC samples, repeat the experiment 10 times and plot the mean estimate with plt.plot and the standard deviation of the estimate using plt.fill_between. [1 mark]\nRepeat (i.) using scipy.integrate.quad and compare it with the estimates obtained in (i.) using a similar plot used in (i.). [0.5 mark]\n\n[1.5 marks] Inverse CDF sampling for Cauchy distribution:\n\nAnalytically derive the Inverse CDF from the CDF of the Cauchy distribution. [0.5 mark]\nDraw samples from the Cauchy distribution (loc=0, scale=1) with inverse CDF sampling. Use the inverse CDF derived in (i.). While generating samples from the uniform distribution, restrict the samples to be between 0.05 and 0.95 to avoid numerical instability. Verify that drawn samples are correct by plotting the kernel density estimation (empirical pdf) with sns.kdeplot (sns stands for the seaborn library) along with pdf obtained with dist.log_prob, where dist is torch.distributions.Cauchy(loc=0, scale=1) [0.5 mark]\nRepeat (ii.) using inverse CDF from torch.distributions.Cauchy. You can access the inverse CDF at dist.icdf where dist is an instance of torch.distributions.Cauchy. [0.5 mark]\n\n[1.5 marks] Rejection sampling:\n\nSample the unnormalized distribution shown in the code below using rejection sampling. Use Normal(loc=5, scale=5), Uniform(-15, 15), and Laplace distribution(loc=5, scale=5) as the proposal distributions. Report the accepance ratios for each proposal distribution (You may choose suitable Multiplier value (M) while considering the support -15 &lt; x &lt; 15). [1 mark]\nCreate and compare plots showing the target distribution (taget_pdf function), proposal distribution (pdf via log_prob method), scaled proposal distribution (scaled by M), and pdf of final normalized target distribution (empirical pdf) using sns.kdeplot. [0.5 mark]\n\n\nimport torch\nimport torch.distributions as D\n\ndef target_pdf(x):\n    gaussian_pdf = D.Normal(0, 1.5).log_prob(x).exp()\n    cauchy_pdf = D.Cauchy(5, 3).log_prob(x).exp()\n    return 0.5 * gaussian_pdf + 0.7 * cauchy_pdf\n\n[3.5 marks] Generate the following classification dataset:\n\nfrom sklearn.datasets import make_circles\nX, y = make_circles(n_samples=100, noise=0.02, random_state=42)\nWe want to perform classification with the following Neural Network which returns logits for the cross-entropy loss:\nimport torch.nn as nn\nmodel = nn.Sequential(\n    nn.Linear(2, 8),\n    nn.ReLU(),\n    nn.Linear(8, 1)\n)\n\nWhat are the total number of parameters in this model? Apply \\(\\mathcal{N}(0, 1)\\) prior on all the parameters of the neural network and find MAP estimate of parameters and plot the predicted probability surface on a dense grid along with data (see this example for code reference). You may choose the appropriate limits for the grid to cover the data with some additional margin. [0.5 mark]\nWhat is the expected size of the Hessian matrix? Compute Full Hessian and invert it to get the covariance matrix. Visualize the heatmap of this covariance matrix with seaborn or matplotlib. A valid covariance matrix is always positive semi-definite (PSD). Check if the obtained covariance matrix is PSD. [1.5 marks]\nAs an approximation, we can ignore the off-diagonal elements of Hessian. Why such a matrix might be easier to invert? After inverting and getting the covariance matrix, visualize it with a heatmap and compare it with the Full covariance obtained in (ii). [0.5 mark]\nSample parameters from the posterior distribution. Use these parameters to obtain the predictive posterior with Monte Carlo sampling on a uniform 2d grid. Plot mean and standard deviation surfaces side-by-side with something similar to plt.subplots(). What are your observations? [1 mark]\n\nFollowing functions from hamiltorch will become useful while working on this:\n# flatten function returns a 1d tensor of parameters from the model.\nflat_params = hamiltorch.util.flatten(model)\n\n# unflatten function returns a list of parameters which has\n# the same structure as list(model.parameters())\nparams_list = hamiltorch.util.unflatten(model, flat_params)\n\n# Put back the weights in the model in-place\nhamiltorch.util.update_model_params_in_place(model, params_list)"
  },
  {
    "objectID": "assignments/2.html",
    "href": "assignments/2.html",
    "title": "Assignment 2 (released 22 Aug, due 30 Aug)",
    "section": "",
    "text": "Total marks: 8\nUse torch for the assignment\nFor distributions use torch.distributions and do not use torch.random directly\nThe assignment has to be done in groups of two.\nThe assignment should be a single Jupyter notebook.\nThe results from every question of your assignment should be in visual formats such as plots, tables. Don’t show model’s log directly in Viva. All plots should have labels and legends appropriately. If not done, we may cut some marks for presentation (e.g. 10%).\nTo know more about a distribution, just look at the Wikipedia page."
  },
  {
    "objectID": "assignments/2.html#instructions",
    "href": "assignments/2.html#instructions",
    "title": "Assignment 2 (released 22 Aug, due 30 Aug)",
    "section": "",
    "text": "Total marks: 8\nUse torch for the assignment\nFor distributions use torch.distributions and do not use torch.random directly\nThe assignment has to be done in groups of two.\nThe assignment should be a single Jupyter notebook.\nThe results from every question of your assignment should be in visual formats such as plots, tables. Don’t show model’s log directly in Viva. All plots should have labels and legends appropriately. If not done, we may cut some marks for presentation (e.g. 10%).\nTo know more about a distribution, just look at the Wikipedia page."
  },
  {
    "objectID": "assignments/2.html#questions",
    "href": "assignments/2.html#questions",
    "title": "Assignment 2 (released 22 Aug, due 30 Aug)",
    "section": "Questions",
    "text": "Questions\n\n[Total marks: 2.5] Consider the dataset below (1.1). Find MLE estimate for parameters of a neural network for regression with Gaussian Homoskedastic noise, where noise variance has a fixed value = 0.0025. Your model summary should match with (1.2). Animate the MLE fit on the data along with the 95% noise variance intervals [2 marks]. What is the effect of varying the noise variance (only in model, not for regenerating the data) on the MLE fit, show it for 3 different noise variance values? [0.5 mark] Refer to this tutorial for building and training torch.nn models. Use FuncAnimation from matplotlib or Celluloid for animation.\n\n1.1 Data Generation\nimport numpy as np\n\nnp.random.seed(0)\nX = np.linspace(0, 1, 100)\nnoise = np.random.normal(0, 0.05, 100)\ny = np.sin(2 * np.pi * X) + noise\n1.2 Model Summary\nmodel = ... # Your model\n\nfrom torchsummary import summary\nsummary(model, (1,))\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Linear-1                   [-1, 10]              20\n              GELU-2                   [-1, 10]               0\n            Linear-3                   [-1, 10]             110\n              SELU-4                   [-1, 10]               0\n            Linear-5                    [-1, 1]              11\n================================================================\nTotal params: 141\nTrainable params: 141\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.00\nEstimated Total Size (MB): 0.00\n----------------------------------------------------------------\n\n[Total marks: 1.5] You toss a coin 10 times and the result turns out to be: [1, 0, 0, 1, 1, 0, 1, 0, 0, 0]. Find the MAP estimate for probability of heads if:\n\nPrior is Beta distribution with parameters (alpha=2, beta=3):\n\nCalculate the answer analytically using the closed form MAP estimate [0.5 mark]\nFind the answer with gradient descent using torch.optim [0.5 mark]\n\nPrior is a Gaussian distribution with mean=0.5 and variance=0.1. Find the answer with gradient descent using torch.optim [0.5 mark]\n\n[Total marks: 2.5] Generate a linear trend dataset with the following code (3.1). Find MAP estimate for slope and intercept:\n\nPrior is Normal distribution with mean=0 and variance=1 [1 marks]\nShow the effect of varying the size of the dataset on the MAP estimate [0.5 mark]\nShow the effect of varying the prior variance on the MAP estimate [0.5 mark]\nChange the prior to Laplace with mean = 0 and scale = 1. Compare the MAP estimate with the one obtained in (i). What do you observe? [0.5 mark]\n\n\n3.1 Generate a linear trend dataset\nimport numpy as np\nslope = 3\nintercept = 2\nN = 100\nX = np.linspace(0, 1, N)\nnoise = np.random.normal(0, 0.05, N)\ny = slope * X + intercept + noise\n\n[Total marks: 1.5] Generate a classification dataset with the following code (4.1). Find the MAP estimate for the parameters of logistic regression. Assume Normal prior with mean=0 and variance=0.1 for all parameters. Visualize the MLE and MAP classification boundaries. What do you observe? [1.5 marks]\n\n4.1 Generate a classification dataset\nfrom sklearn.datasets import make_blobs\n\nX, y = make_blobs(\n    n_samples=100, centers=2, n_features=2, random_state=0, cluster_std=0.8\n)"
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Prerequisites"
  },
  {
    "objectID": "grading.html",
    "href": "grading.html",
    "title": "Grading Policy",
    "section": "",
    "text": "TO BE FINALISED\n\nQuizzes (36%): Three quizzes of 12% each. These are held in the exam 1, 2, 3 slots.\nAssignments (32%):\n\nFour assignments of ~8% each.\nFollowed by a viva.\nTotal number of groups in the course would be &lt;=20. Depending on class size, the groups would be of 2 or 3 students.\nThe assignments would be graded on the basis of the following rubric:\n\n50%: Correctness of the code\n25%: Code quality\n25%: Presentation/Conceptual understanding\n\nYou get one week to complete each assignment.\nTentative schedule:\n\nAssignment 1: 10 August (due 18 August)\nAssignment 2: 21 August (due 28 August)\nAssignment 3: 14 September (due 21 September)\nAssignment 4: 30 October (due 6 November) Assignment 4: 28 October (due 8 November)\n\n\nProject (27%): The number of project groups would be &lt;=10. Depending on the class size, the groups would be of 3-5 students.\n\nTentative schedule:\n\nPresentation I: 6 October (10%)\nFinal Project presentation: 22 November 17 Nov (12%)\nFinal report: 24 November 20 Nov (5%)\n\n\nInteractive article (5%): Create a Streamlit or similar interactive article for concept explanation (due 15 November)"
  },
  {
    "objectID": "assignments/1.html",
    "href": "assignments/1.html",
    "title": "Assignment 1 (released 10 Aug, due 18 Aug)",
    "section": "",
    "text": "Total marks: 6\nUse torch for the assignment\nFor distributions use torch.distributions and do not use torch.random directly\nThe assignment has to be done in groups of two.\nThe assignment should be a single Jupyter notebook.\nThe results from every question of your assignment should be in visual formats such as plots, tables. Don’t show model’s log directly in Viva. All plots should have labels and legends appropriately. If not done, we may cut some marks for presentation (e.g. 10%).\nTo know more about a distribution, just look at the Wikipedia page."
  },
  {
    "objectID": "assignments/1.html#instructions",
    "href": "assignments/1.html#instructions",
    "title": "Assignment 1 (released 10 Aug, due 18 Aug)",
    "section": "",
    "text": "Total marks: 6\nUse torch for the assignment\nFor distributions use torch.distributions and do not use torch.random directly\nThe assignment has to be done in groups of two.\nThe assignment should be a single Jupyter notebook.\nThe results from every question of your assignment should be in visual formats such as plots, tables. Don’t show model’s log directly in Viva. All plots should have labels and legends appropriately. If not done, we may cut some marks for presentation (e.g. 10%).\nTo know more about a distribution, just look at the Wikipedia page."
  },
  {
    "objectID": "assignments/1.html#questions",
    "href": "assignments/1.html#questions",
    "title": "Assignment 1 (released 10 Aug, due 18 Aug)",
    "section": "Questions",
    "text": "Questions\n\nOptimise the following function using torch autograd and gradient descent, f(θ) = (θ₀ - 2)² + (θ₁ - 3)². In addition to finding the optima, you need to show the convergence plots. [0.5 marks]\nGenerate some data (100 data points) using a univariate Normal distribution with loc=2.0 and scale=4.0.\n\nPlot a 2d contour plot showing the Likelihood or the Log-Likelihood as a function of loc and scale. Please label all the axes including the colorbar. [1 mark]\nFind the MLE parameters for the loc and scale using gradient descent. Plot convergence plot as well. [1 mark]\nRedo the above question but learn log(scale) instead of scale and then finally transform to learn scale. What can you conclude? Why is this transformation useful? [0.5 mark]\n\nGenerate some data (1000 data points) using a univariate Normal distribution with loc=2.0 and scale=4.0 and using Student-T distributions with varying degrees (from 1-8) of freedom (1000 data points corresponding to each degree of freedom). Plot the pdf (and logpdf) at uniformly spaced data from (-50, 50) in steps of 0.1. What can you conclude? [1 mark]\nAnalytically derive the MLE for exponential distribution. Generate some data (1000 data points) using some fixed parameter values and see if you can recover the analytical parameters using gradient descent based solution for obtaining MLE. [1 mark]\nGenerate some data (100 data points) using a univariate Normal distribution with loc=2.0 and scale=4.0. Now, create datasets of size 10, 20, 50, 100, 500, 1000, 5000, 10000. We will use a different random seed to create ten different datasets for each of these sizes. For each of these datasets, find the MLE parameters for the loc and scale using gradient descent. Plot the estimates of loc and scale as a function of the dataset size. What can you conclude? [1 mark]"
  },
  {
    "objectID": "assignments/4.html",
    "href": "assignments/4.html",
    "title": "Assignment 4 (released 28 Oct, due 8 Nov 7 PM)",
    "section": "",
    "text": "Total Marks: 9\nUse torch for the assignment.\nFor sampling, use torch.distributions and do not use torch.random directly.\nThe assignment has to be done in groups of two.\nThe assignment should be a single jupyter notebook.\nThe results from every question of your assignment should be in visual formats such as plots and tables. Don’t show model’s log directly in Viva. All plots should have labels and legends appropriately. If not done, we may cut some marks for presentation (e.g. 10%)."
  },
  {
    "objectID": "assignments/4.html#instructions",
    "href": "assignments/4.html#instructions",
    "title": "Assignment 4 (released 28 Oct, due 8 Nov 7 PM)",
    "section": "",
    "text": "Total Marks: 9\nUse torch for the assignment.\nFor sampling, use torch.distributions and do not use torch.random directly.\nThe assignment has to be done in groups of two.\nThe assignment should be a single jupyter notebook.\nThe results from every question of your assignment should be in visual formats such as plots and tables. Don’t show model’s log directly in Viva. All plots should have labels and legends appropriately. If not done, we may cut some marks for presentation (e.g. 10%)."
  },
  {
    "objectID": "assignments/4.html#questions",
    "href": "assignments/4.html#questions",
    "title": "Assignment 4 (released 28 Oct, due 8 Nov 7 PM)",
    "section": "Questions",
    "text": "Questions\n\n[1 mark] Implement Logistic Regression using the Pyro library referring [1] for guidance. Show both the mean prediction as well as standard deviation in the predictions over the 2d grid. Use NUTS MCMC sampling to sample the posterior. Take 1000 samples for posterior distribution and use 500 samples as burn/warm up. Use the below given dataset.\n\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_moons(n_samples=100, noise=0.3, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=42)\n\n[2 marks] Consider the FVC dataset example discussed in the class. Find the notebook link at [2]. We had only used the train dataset. Now, we want to find out the performance of various models on the test dataset. Use the given dataset and deduce which model works best in terms of error (MAE) and coverage? The base model is Linear Regression by Sklearn (from sklearn.linear_model import LinearRegression). Plot the trace diagrams and posterior distribution. Also plot the predictive posterior distribution with 90% confidence interval.\n\nx_train, x_test, y_train, y_test = train_test_split(train[\"Weeks\"], train['FVC'], train_size = 0.8, random_state = 0)\n\n\n\n\n\n\nPooled Model Older (wrong) version\n\n\n\n \\[\\begin{align*}\n\\alpha &\\sim \\mathcal{N}(0, 500) \\\\\n\\beta &\\sim \\mathcal{N}(0, 500) \\\\\n\\sigma &\\sim \\text{HalfNormal}(100)\n\\end{align*}\\] \\[\\begin{equation*}\nFVC_j \\sim \\text{Normal}(\\alpha + \\beta \\cdot Week_, \\sigma)\n\\end{equation*}\\] where \\(j\\) is the patient index.\n\n\n\n\n\n\n\n\nPooled Model Newer (corrected) version\n\n\n\n \\[\\begin{align*}\n\\alpha &\\sim \\mathcal{N}(0, 500) \\\\\n\\beta &\\sim \\mathcal{N}(0, 500) \\\\\n\\sigma &\\sim \\text{HalfNormal}(100)\n\\end{align*}\\] \\[\\begin{equation*}\nFVC_j \\sim \\text{Normal}(\\alpha + \\beta \\cdot Week_j, \\sigma)\n\\end{equation*}\\] where \\(j\\) is the week index.\n\n\n\n\n\n\n\n\n\nPartially pooled model with the same sigma\n\n\n\n\\[\\begin{align*}\n\\mu_\\alpha &\\sim \\mathcal{N}(0, 500) \\\\\n\\sigma_\\alpha &\\sim \\text{HalfNormal}(100) \\\\\n\\mu_\\beta &\\sim \\mathcal{N}(0, 3) \\\\\n\\sigma_\\beta &\\sim \\text{HalfNormal}(3) \\\\\n\\alpha_i &\\sim \\mathcal{N}(\\mu_\\alpha, \\sigma_\\alpha) \\\\\n\\beta_i &\\sim \\mathcal{N}(\\mu_\\beta, \\sigma_\\beta) \\\\\n\\sigma &\\sim \\text{HalfNormal}(100)\n\\end{align*}\\]\n\\[\\begin{equation*}\nFVC_{ij} \\sim \\text{Normal}(\\alpha_i + \\beta_i \\cdot Week_j, \\sigma)\n\\end{equation*}\\] where \\(i\\) is the patient index and \\(j\\) is the week index.\n\n\n\n\n\n\n\n\nPartially pooled model with the sigma hyperpriors\n\n\n\n \\[\\begin{align*}\n\\mu_\\alpha &\\sim \\mathcal{N}(0, 500) \\\\\n\\sigma_\\alpha &\\sim \\text{HalfNormal}(100) \\\\\n\\mu_\\beta &\\sim \\mathcal{N}(0, 3) \\\\\n\\sigma_\\beta &\\sim \\text{HalfNormal}(3) \\\\\n\\alpha_i &\\sim \\mathcal{N}(\\mu_\\alpha, \\sigma_\\alpha) \\\\\n\\beta_i &\\sim \\mathcal{N}(\\mu_\\beta, \\sigma_\\beta) \\\\\n\\gamma_\\sigma &\\sim \\text{HalfNormal}(30) \\\\\n\\sigma_i &\\sim \\text{Exp}(\\gamma_\\sigma)\n\\end{align*}\\]\n\\[\\begin{equation*}\nFVC_{ij} \\sim \\text{Normal}(\\alpha_i + \\beta_i \\cdot Week_j, \\sigma_i)\n\\end{equation*}\\] where \\(i\\) is the patient index and \\(j\\) is the week index.\n\n\n\n[4 marks] Use your version of following models to reproduce figure 4 from the paper referenced at [2]. You can also refer to the notebook in the course.\n\nHypernet [2 marks]\nNeural Processes [2 marks]\n\n[2 marks] Write the Random walk Metropolis Hastings algorithms from scratch. Take 1000 samples using below given log probs and compare the mean and covariance matrix with hamiltorch’s standard HMC and emcee’s Metropolis Hastings implementation. Use 500 samples as the burn/warm up samples. Also check the relation between acceptance ratio and the sigma of the proposal distribution in your from scratch implementation. Use the log likelihood function given below.\n\nimport torch.distributions as D\ndef log_likelihood(omega):\n    omega = torch.tensor(omega)\n    mean = torch.tensor([0., 0.])\n    stddev = torch.tensor([0.5, 1.])\n    return D.MultivariateNormal(mean, torch.diag(stddev**2)).log_prob(omega).sum()"
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "Notebook",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "notebooks/bayes-librarian.html",
    "href": "notebooks/bayes-librarian.html",
    "title": "ES661 PML",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\np_l = 1.0/21.0\np_f = 1- p_l\nprior = torch.distributions.Categorical(probs=torch.tensor([p_l, p_f]))\nprior\n\nCategorical(probs: torch.Size([2]))\n\n\n\npd.Series(index=[\"Librarian\", \"Farmer\"], data=prior.probs).plot(kind='bar', rot=0)\n# Write the values on top of the bar\nfor i, v in enumerate(prior.probs):\n    plt.text(i - 0.1, v + 0.01, f\"{v.item():0.3f}\")\nplt.title(\"Prior\")\n\nText(0.5, 1.0, 'Prior')\n\n\n\n\n\n\n# Get 210 samples from prior given a seed\n\ntorch.manual_seed(3)\nsamples = prior.sample(torch.Size([210, ]))\nprint(samples)\nprint(samples.sum())\n\ntensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\ntensor(199)\n\n\n\np_d_l = torch.tensor(0.4)\np_d_f = torch.tensor(0.1)\n\n\np_l_d = (p_d_l * p_l) / (p_d_l * p_l + p_d_f * p_f)\np_l_d\n\ntensor(0.1667)\n\n\n\np_d_l *p_l\n\ntensor(0.0190)\n\n\n\np_d_f*p_f\n\ntensor(0.0952)\n\n\n\n## Electricity consumption\n\n\nprior_theta_0 = torch.distributions.Uniform(80, 100)\nprior_theta_1 = torch.distributions.Uniform(1, 2)\n\n\n# Plottting PDFs\n\neps = 1e-3\ntheta_0_lin = torch.linspace(80+eps, 100.0-eps, 100)\nplt.plot(theta_0_lin, torch.exp(prior_theta_0.log_prob(theta_0_lin)))\nplt.xlabel(r'$\\theta_0$')\nplt.ylabel(r'$p(\\theta_0)$')\nplt.title(r'Prior PDF for $\\theta_0$')\n# Fill between \nplt.fill_between(theta_0_lin, torch.exp(prior_theta_0.log_prob(theta_0_lin)), color='C0', alpha=0.2)\n\n&lt;matplotlib.collections.PolyCollection at 0x7f67945e25b0&gt;\n\n\n\n\n\n\ntheta_1_lin = torch.linspace(1+eps, 2.0-eps, 100)\nplt.plot(theta_1_lin, torch.exp(prior_theta_1.log_prob(theta_1_lin)))\nplt.xlabel(r'$\\theta_1$')\nplt.ylabel(r'$p(\\theta_1)$')\nplt.title(r'Prior PDF for $\\theta_1$')\n# Fill between \nplt.fill_between(theta_1_lin, torch.exp(prior_theta_1.log_prob(theta_1_lin)), color='C0', alpha=0.2)\n\n&lt;matplotlib.collections.PolyCollection at 0x7f67923c04c0&gt;\n\n\n\n\n\n\n# plotting samples from prior\n\nn_samples = 1000\ntheta_0s = prior_theta_0.sample(torch.Size([n_samples,]))\ntheta_1s = prior_theta_1.sample(torch.Size([n_samples,]))\n\n\n# Plot the samples\nfig, ax = plt.subplots(figsize=(8, 2), ncols=2)\nax[0].plot(theta_0_lin, torch.exp(prior_theta_0.log_prob(theta_0_lin)), label='prior')\n\n# plot theta_0 samples\nax[0].scatter(theta_0s, torch.zeros_like(theta_0s),  label='samples',c='r', marker='x')\n\nax[1].plot(theta_1_lin, torch.exp(prior_theta_1.log_prob(theta_1_lin)), label='prior')\nax[1].scatter(theta_1s, torch.zeros_like(theta_1s), label='samples',c='r', marker='x')\n\nax[1].set_title(r'$\\theta_1$')\nax[0].set_title(r'$\\theta_0$')\n\nText(0.5, 1.0, '$\\\\theta_0$')\n\n\n\n\n\n\n# Generating draws from prior\n\nxs = torch.linspace(1, 1000, 100)\ndef forward(x, theta_0, theta_1):\n    return theta_0 + theta_1 * x\n\npreds = []\n\nfor theta_0, theta_1 in zip(theta_0s, theta_1s):\n    print(theta_0.item(), theta_1.item())\n    preds.append(forward(xs, theta_0, theta_1))\n    plt.plot(xs, preds[-1], color='C1', alpha=0.5)\n\nplt.xlabel('Number of Occupants')\nplt.ylabel('Electricity Consumption')\n\n87.69513702392578 1.0412778854370117\n95.6269760131836 1.1212730407714844\n86.8476333618164 1.2125056982040405\n90.94928741455078 1.1671473979949951\n96.58927154541016 1.3385244607925415\n87.47602844238281 1.558532476425171\n88.01754760742188 1.1025707721710205\n90.33709716796875 1.9295198917388916\n88.80725860595703 1.9910452365875244\n98.52078247070312 1.986382007598877\n84.97046661376953 1.1545259952545166\n89.32050323486328 1.5447709560394287\n96.62725067138672 1.250910997390747\n81.65365600585938 1.5428019762039185\n89.25396728515625 1.5592520236968994\n85.2579574584961 1.0442684888839722\n94.31983947753906 1.6553218364715576\n89.50067138671875 1.9270992279052734\n95.697998046875 1.299673318862915\n98.55661010742188 1.488832950592041\n85.66251373291016 1.7670807838439941\n84.5622787475586 1.9460041522979736\n89.70915222167969 1.7484333515167236\n83.01439666748047 1.6918725967407227\n94.00970458984375 1.6238453388214111\n80.7804946899414 1.3233580589294434\n91.94137573242188 1.953773021697998\n97.56145477294922 1.169715166091919\n92.48450469970703 1.114927887916565\n83.50093078613281 1.102515459060669\n82.62401580810547 1.5394084453582764\n82.85623931884766 1.4271032810211182\n86.58822631835938 1.0481557846069336\n94.35069274902344 1.1144349575042725\n91.43531799316406 1.2319121360778809\n90.06417083740234 1.374377727508545\n99.1529541015625 1.220672369003296\n92.55977630615234 1.6680750846862793\n94.7859115600586 1.1695594787597656\n81.30776977539062 1.3169939517974854\n90.8858413696289 1.424668550491333\n94.4291000366211 1.144008755683899\n86.97049713134766 1.876807689666748\n98.18754577636719 1.5776394605636597\n92.92131042480469 1.9428175687789917\n80.43154907226562 1.4329195022583008\n85.76531219482422 1.875524878501892\n87.48646545410156 1.3303887844085693\n87.47976684570312 1.428504228591919\n86.83807373046875 1.197716474533081\n86.94332885742188 1.5565887689590454\n94.37505340576172 1.4149739742279053\n93.06510162353516 1.067138671875\n96.40441131591797 1.289334774017334\n86.10169982910156 1.5684490203857422\n90.42550659179688 1.2710241079330444\n91.0185775756836 1.0674898624420166\n93.92642211914062 1.3713047504425049\n86.58151245117188 1.1188983917236328\n85.77822875976562 1.5820157527923584\n92.26363372802734 1.3258450031280518\n99.87332916259766 1.6146454811096191\n84.66149139404297 1.1545571088790894\n82.93559265136719 1.6960713863372803\n89.84662628173828 1.5519955158233643\n96.96514892578125 1.7468791007995605\n95.19371795654297 1.976472020149231\n94.27191925048828 1.6630446910858154\n82.7536392211914 1.9746594429016113\n90.74134826660156 1.8186657428741455\n99.88572692871094 1.354374647140503\n93.4601821899414 1.410269856452942\n90.59257507324219 1.2586917877197266\n87.16841888427734 1.2511789798736572\n97.9290542602539 1.339185357093811\n95.5967788696289 1.7606523036956787\n92.15022277832031 1.3517167568206787\n84.46255493164062 1.1570004224777222\n90.3648681640625 1.0489882230758667\n86.80931091308594 1.6851460933685303\n96.78753662109375 1.3440935611724854\n93.14493560791016 1.7365984916687012\n98.11967468261719 1.602406620979309\n80.8687744140625 1.038487195968628\n94.12189483642578 1.223759651184082\n91.72600555419922 1.6831367015838623\n89.2578353881836 1.2158634662628174\n80.0279769897461 1.6264379024505615\n99.61504364013672 1.084007978439331\n83.5066909790039 1.5729857683181763\n80.53050994873047 1.8047398328781128\n89.59612274169922 1.953641653060913\n92.02345275878906 1.1013636589050293\n95.53692626953125 1.4922316074371338\n91.98695373535156 1.2603998184204102\n90.43048095703125 1.0833650827407837\n94.84186553955078 1.9851264953613281\n87.72840118408203 1.065014123916626\n81.46113586425781 1.6820820569992065\n97.96985626220703 1.234586238861084\n82.44818878173828 1.011096477508545\n96.11310577392578 1.4245643615722656\n87.82499694824219 1.6904051303863525\n84.83235931396484 1.1519567966461182\n88.76789093017578 1.13896644115448\n84.79768371582031 1.312787413597107\n80.25230407714844 1.1010459661483765\n95.03224182128906 1.7609448432922363\n81.25543212890625 1.04115629196167\n89.00857543945312 1.1022148132324219\n92.93892669677734 1.6987924575805664\n93.81294250488281 1.2739479541778564\n91.67818450927734 1.8880064487457275\n82.468994140625 1.276659607887268\n94.61796569824219 1.2439587116241455\n80.1130142211914 1.0376932621002197\n97.12281036376953 1.730310082435608\n92.46382141113281 1.7947566509246826\n90.61416625976562 1.8840017318725586\n82.46800231933594 1.7793362140655518\n96.13663482666016 1.5259487628936768\n96.89361572265625 1.305039405822754\n89.8781509399414 1.0746833086013794\n91.61681365966797 1.7047715187072754\n82.93636322021484 1.238344430923462\n82.81407928466797 1.0890676975250244\n96.61282348632812 1.5943901538848877\n99.01074981689453 1.4189800024032593\n81.95106506347656 1.5043325424194336\n96.00111389160156 1.1587986946105957\n93.47216796875 1.2459975481033325\n98.85737609863281 1.9700062274932861\n86.61084747314453 1.6007869243621826\n95.08634948730469 1.1321947574615479\n84.5699234008789 1.842087745666504\n90.51310729980469 1.68861985206604\n87.24632263183594 1.941713571548462\n82.32901000976562 1.665165901184082\n95.54249572753906 1.6798239946365356\n84.75167846679688 1.8800559043884277\n89.34307098388672 1.231102466583252\n86.72317504882812 1.9233851432800293\n89.29837036132812 1.8016605377197266\n88.3612060546875 1.37612783908844\n98.79486083984375 1.523589849472046\n98.48333740234375 1.2476298809051514\n97.07954406738281 1.0195120573043823\n89.96569061279297 1.0555812120437622\n80.99148559570312 1.376490592956543\n93.362060546875 1.7271687984466553\n81.7196044921875 1.5611259937286377\n90.10861206054688 1.8441978693008423\n89.50457000732422 1.5818358659744263\n84.30509948730469 1.2079615592956543\n83.50444793701172 1.7497899532318115\n91.68502044677734 1.3454217910766602\n80.13153839111328 1.0732593536376953\n99.80758666992188 1.6212793588638306\n91.77196502685547 1.2740830183029175\n94.59294128417969 1.097072720527649\n88.84345245361328 1.4506481885910034\n86.78339385986328 1.0639290809631348\n88.83187866210938 1.8970956802368164\n94.25908660888672 1.587091088294983\n88.65489196777344 1.1026930809020996\n82.4282455444336 1.9630863666534424\n97.47400665283203 1.292677402496338\n96.66935729980469 1.2211899757385254\n81.74362182617188 1.0532464981079102\n89.22825622558594 1.754408597946167\n82.65867614746094 1.0193803310394287\n88.86253356933594 1.1457579135894775\n80.92573547363281 1.0950336456298828\n94.04350280761719 1.990125060081482\n96.60394287109375 1.987403154373169\n97.53553771972656 1.4419186115264893\n89.57406616210938 1.6335155963897705\n91.17781066894531 1.4893226623535156\n86.52110290527344 1.6093614101409912\n86.1540756225586 1.9536876678466797\n87.56365203857422 1.566344976425171\n90.33023071289062 1.8433496952056885\n82.40996551513672 1.0289032459259033\n85.71919250488281 1.919156551361084\n99.86544036865234 1.782569408416748\n94.81980895996094 1.4061675071716309\n82.9118881225586 1.4045815467834473\n99.01624298095703 1.5185314416885376\n97.09477233886719 1.645193338394165\n89.12952423095703 1.5449402332305908\n88.90231323242188 1.1528569459915161\n87.5825424194336 1.441185712814331\n87.4133529663086 1.800107479095459\n92.37303161621094 1.5086145401000977\n99.30390167236328 1.9642198085784912\n99.74899291992188 1.4058808088302612\n95.60200500488281 1.5114103555679321\n90.23295593261719 1.444563627243042\n92.5718002319336 1.5712342262268066\n81.01299285888672 1.8754684925079346\n89.2287368774414 1.7894538640975952\n92.5677719116211 1.8815683126449585\n94.48453521728516 1.0842392444610596\n93.99372100830078 1.7238528728485107\n91.33424377441406 1.1637542247772217\n88.50811767578125 1.3193455934524536\n85.53105926513672 1.6868886947631836\n87.34080505371094 1.8577947616577148\n92.7117919921875 1.5039350986480713\n89.24696350097656 1.3948745727539062\n81.40795135498047 1.086325764656067\n96.5345458984375 1.7923710346221924\n98.70243835449219 1.9552621841430664\n92.06018829345703 1.6399625539779663\n87.05399322509766 1.1945961713790894\n81.01774597167969 1.5529268980026245\n98.87288665771484 1.5917402505874634\n89.5815658569336 1.7662642002105713\n82.79290008544922 1.4998633861541748\n80.24394989013672 1.1864984035491943\n95.508056640625 1.1963965892791748\n86.65911865234375 1.5199213027954102\n84.08351135253906 1.8646318912506104\n92.0901870727539 1.1967954635620117\n88.18511199951172 1.326673150062561\n94.06657409667969 1.9710323810577393\n85.5030288696289 1.3380656242370605\n90.45893096923828 1.2146241664886475\n81.58501434326172 1.865027666091919\n96.42964935302734 1.1058900356292725\n98.82797241210938 1.1475470066070557\n98.73994445800781 1.4258317947387695\n94.16300201416016 1.33109450340271\n98.30198669433594 1.634709358215332\n93.94081115722656 1.828623652458191\n86.51219177246094 1.265526294708252\n96.52320861816406 1.8763457536697388\n98.6265869140625 1.1988688707351685\n91.43827819824219 1.8440756797790527\n93.32282257080078 1.0980292558670044\n81.95563507080078 1.354668140411377\n96.27238464355469 1.215299129486084\n84.34428405761719 1.8884146213531494\n81.55213165283203 1.5654256343841553\n89.6497802734375 1.356525182723999\n99.29093933105469 1.6838717460632324\n95.04837799072266 1.3560669422149658\n81.63878631591797 1.2418384552001953\n89.12718963623047 1.3357279300689697\n87.46515655517578 1.4678325653076172\n92.83613586425781 1.7892435789108276\n91.69847106933594 1.2009649276733398\n98.4359130859375 1.905756950378418\n98.43559265136719 1.0245305299758911\n87.91726684570312 1.7803137302398682\n93.9311752319336 1.4504296779632568\n97.53208923339844 1.7488648891448975\n85.20597839355469 1.3056707382202148\n99.80611419677734 1.404740333557129\n88.9251480102539 1.0782647132873535\n85.16576385498047 1.3475453853607178\n85.73490905761719 1.4886318445205688\n91.07793426513672 1.8859808444976807\n81.94158935546875 1.5911576747894287\n87.99101257324219 1.6303831338882446\n98.05056762695312 1.0774091482162476\n95.1716079711914 1.0707284212112427\n96.35324096679688 1.3814113140106201\n85.59410095214844 1.7233291864395142\n98.8589859008789 1.6704661846160889\n82.66046905517578 1.0010724067687988\n82.14085388183594 1.3173435926437378\n96.20088195800781 1.5716652870178223\n97.9405746459961 1.7548600435256958\n90.15998840332031 1.0804041624069214\n94.58312225341797 1.360841155052185\n95.50540161132812 1.0990573167800903\n92.21611022949219 1.5698652267456055\n99.58377075195312 1.6420044898986816\n85.63987731933594 1.9295915365219116\n96.26022338867188 1.0003501176834106\n80.15142822265625 1.0048238039016724\n89.23384094238281 1.9587490558624268\n82.99113464355469 1.1053338050842285\n97.3426284790039 1.5909531116485596\n98.71217346191406 1.1090104579925537\n87.20555114746094 1.298750400543213\n89.35206604003906 1.1972639560699463\n83.49285888671875 1.1133019924163818\n85.85491180419922 1.869433879852295\n88.74373626708984 1.1152124404907227\n84.66169738769531 1.032759189605713\n93.00992584228516 1.9837510585784912\n86.14134979248047 1.6887331008911133\n98.59461975097656 1.080071210861206\n94.73643493652344 1.6711037158966064\n87.08386993408203 1.4622790813446045\n80.96151733398438 1.0804364681243896\n93.2742691040039 1.9616378545761108\n97.86018371582031 1.6221661567687988\n90.74947357177734 1.4679657220840454\n89.17950439453125 1.6991548538208008\n82.93622589111328 1.1061826944351196\n83.94174194335938 1.1872522830963135\n81.08946990966797 1.8086113929748535\n87.80683898925781 1.305240511894226\n85.30784606933594 1.4780648946762085\n80.12765502929688 1.897214412689209\n92.23592376708984 1.33310866355896\n80.84759521484375 1.6072919368743896\n91.5848159790039 1.220404863357544\n96.11167907714844 1.2652465105056763\n92.53817749023438 1.931967854499817\n96.32613372802734 1.3347055912017822\n80.47168731689453 1.7610195875167847\n89.83350372314453 1.9038317203521729\n82.05730438232422 1.3888483047485352\n97.04126739501953 1.524307370185852\n99.83863830566406 1.1387238502502441\n96.62197875976562 1.5320913791656494\n86.57617950439453 1.496248722076416\n90.88282775878906 1.7337374687194824\n84.61872863769531 1.6061371564865112\n80.97779083251953 1.2528002262115479\n80.44483184814453 1.516909122467041\n95.21382904052734 1.9045822620391846\n80.2861328125 1.79441237449646\n94.36021423339844 1.8566420078277588\n92.70948791503906 1.445806622505188\n89.04227447509766 1.3563913106918335\n80.06767272949219 1.36397123336792\n82.87981414794922 1.0417895317077637\n85.39154052734375 1.8766453266143799\n97.44548034667969 1.7993073463439941\n90.1553726196289 1.5245330333709717\n83.11884307861328 1.346864104270935\n88.65609741210938 1.6343283653259277\n81.6729507446289 1.8613231182098389\n83.9632797241211 1.8341238498687744\n88.94541931152344 1.6620174646377563\n96.30646514892578 1.333033800125122\n95.3998031616211 1.1219444274902344\n92.66915893554688 1.8882604837417603\n89.97352600097656 1.9781239032745361\n82.77247619628906 1.8393640518188477\n98.02163696289062 1.6135040521621704\n80.08682250976562 1.1931250095367432\n82.48628234863281 1.7100921869277954\n89.97698211669922 1.8217391967773438\n88.32479095458984 1.6070287227630615\n97.6349868774414 1.8012388944625854\n93.0042495727539 1.4907748699188232\n98.78262329101562 1.9023869037628174\n89.17003631591797 1.1349231004714966\n88.15408325195312 1.9487724304199219\n84.39936828613281 1.0591232776641846\n89.317138671875 1.2374107837677002\n88.28820037841797 1.4968205690383911\n85.02552032470703 1.890254259109497\n84.24485778808594 1.065805435180664\n82.170166015625 1.8334324359893799\n86.24152374267578 1.5876519680023193\n97.01067352294922 1.1534284353256226\n81.58153533935547 1.861690640449524\n83.04762268066406 1.3336613178253174\n93.97317504882812 1.5183649063110352\n90.00640869140625 1.5225598812103271\n90.9233169555664 1.1471093893051147\n97.70230865478516 1.851118564605713\n93.2962875366211 1.4258244037628174\n99.33335876464844 1.4624136686325073\n86.70885467529297 1.0074397325515747\n88.84932708740234 1.8021918535232544\n80.50030517578125 1.0101171731948853\n82.77212524414062 1.1637043952941895\n90.78730010986328 1.0748193264007568\n94.13116455078125 1.8970420360565186\n95.94595336914062 1.6449190378189087\n91.89263916015625 1.7218899726867676\n85.97459411621094 1.4877347946166992\n98.50749206542969 1.1829235553741455\n88.4510498046875 1.6825315952301025\n85.79403686523438 1.259835124015808\n88.90570068359375 1.6126017570495605\n93.93443298339844 1.398136854171753\n81.72805786132812 1.9015271663665771\n96.02115631103516 1.0172184705734253\n98.46682739257812 1.3596460819244385\n84.52902221679688 1.6072989702224731\n88.45693969726562 1.2749097347259521\n94.19126892089844 1.0986212491989136\n99.05484008789062 1.971077799797058\n94.07135772705078 1.2463831901550293\n80.08610534667969 1.6148242950439453\n95.32771301269531 1.4674646854400635\n84.63024139404297 1.4844646453857422\n80.96675109863281 1.1639809608459473\n81.53005981445312 1.3308959007263184\n96.05962371826172 1.8497109413146973\n93.70967864990234 1.4482638835906982\n92.92405700683594 1.2706226110458374\n92.90196990966797 1.797845721244812\n83.52706909179688 1.4338971376419067\n94.42025756835938 1.4818401336669922\n95.91780090332031 1.9951822757720947\n97.80067443847656 1.0893971920013428\n89.46163940429688 1.5250924825668335\n92.21309661865234 1.4673281908035278\n81.63961029052734 1.460693120956421\n96.56671142578125 1.8651611804962158\n81.10565948486328 1.668044090270996\n81.5894775390625 1.271575689315796\n88.22618103027344 1.6871442794799805\n98.44947052001953 1.4937515258789062\n90.52273559570312 1.1085081100463867\n80.44692993164062 1.5279107093811035\n87.55543518066406 1.324415683746338\n90.79936218261719 1.1178081035614014\n81.06486511230469 1.736134648323059\n98.48744201660156 1.4704158306121826\n96.25715637207031 1.3769464492797852\n90.72998809814453 1.8688011169433594\n89.17372131347656 1.819145917892456\n82.09491729736328 1.1790657043457031\n85.36598205566406 1.8264503479003906\n95.80296325683594 1.4930779933929443\n95.19683074951172 1.1025617122650146\n91.56018829345703 1.8875255584716797\n95.2552490234375 1.632291555404663\n91.86444091796875 1.569888710975647\n93.09357452392578 1.1392353773117065\n80.02865600585938 1.7590293884277344\n89.14838409423828 1.4920017719268799\n90.33744812011719 1.127612590789795\n97.77242279052734 1.306746006011963\n95.96150207519531 1.6678380966186523\n90.35309600830078 1.7874670028686523\n86.41181945800781 1.7617707252502441\n85.44058227539062 1.6017524003982544\n99.6016616821289 1.7393227815628052\n96.60693359375 1.774422526359558\n99.51984405517578 1.2080415487289429\n94.75322723388672 1.5050280094146729\n97.84530639648438 1.422541856765747\n96.12996673583984 1.2059016227722168\n96.96800231933594 1.0802865028381348\n99.29610443115234 1.3785951137542725\n99.7637939453125 1.155839443206787\n81.2962646484375 1.4671499729156494\n82.70000457763672 1.8565661907196045\n86.53301239013672 1.3617717027664185\n80.9884033203125 1.0914902687072754\n84.16556549072266 1.1358442306518555\n98.1244125366211 1.3115699291229248\n86.32878875732422 1.7048910856246948\n90.72171020507812 1.3039581775665283\n99.68870544433594 1.5448617935180664\n91.55982208251953 1.6279070377349854\n95.13211822509766 1.8350934982299805\n92.51482391357422 1.5801448822021484\n98.24629974365234 1.1688917875289917\n83.68853759765625 1.5509145259857178\n83.41927337646484 1.1064735651016235\n95.55207824707031 1.4728004932403564\n82.78467559814453 1.4583171606063843\n93.66595458984375 1.4533686637878418\n98.6113052368164 1.9356716871261597\n83.80675506591797 1.8612539768218994\n80.73794555664062 1.8096702098846436\n97.62300109863281 1.3299038410186768\n92.72089385986328 1.5748240947723389\n85.024658203125 1.7261515855789185\n93.6810531616211 1.8122947216033936\n80.28606414794922 1.7277923822402954\n82.3918685913086 1.8946515321731567\n98.45598602294922 1.746030569076538\n86.21611022949219 1.358589768409729\n94.80453491210938 1.5360863208770752\n92.11781311035156 1.71842360496521\n86.97250366210938 1.4501370191574097\n90.20829772949219 1.9510061740875244\n95.7842025756836 1.5418438911437988\n91.82008361816406 1.6570210456848145\n87.27477264404297 1.7911310195922852\n81.82063293457031 1.5614025592803955\n92.24595642089844 1.3466110229492188\n99.67198181152344 1.0122146606445312\n86.72737121582031 1.06300950050354\n87.62284851074219 1.8864786624908447\n93.38764953613281 1.3682794570922852\n81.69622039794922 1.9802289009094238\n90.4000473022461 1.8631947040557861\n98.49722290039062 1.4564552307128906\n88.75487518310547 1.655290961265564\n87.91423797607422 1.1321805715560913\n87.37792205810547 1.6390581130981445\n97.37086486816406 1.4559935331344604\n90.47662353515625 1.4960585832595825\n99.66328430175781 1.7768841981887817\n84.1020736694336 1.6165105104446411\n99.58877563476562 1.5559637546539307\n89.61442565917969 1.0251836776733398\n93.586181640625 1.507324457168579\n98.76620483398438 1.0353035926818848\n81.74576568603516 1.7205651998519897\n98.36883544921875 1.42416512966156\n95.49655151367188 1.8263685703277588\n98.14613342285156 1.0789601802825928\n98.68721008300781 1.3878099918365479\n99.5651626586914 1.1787943840026855\n87.18000030517578 1.6864721775054932\n86.595703125 1.9095823764801025\n95.84112548828125 1.06962251663208\n84.1778793334961 1.7237639427185059\n83.77605438232422 1.8962490558624268\n94.51152038574219 1.7772753238677979\n90.28147888183594 1.5205633640289307\n90.34710693359375 1.8482685089111328\n81.21249389648438 1.7342454195022583\n84.42314910888672 1.6690165996551514\n98.13052368164062 1.8424246311187744\n96.65554809570312 1.8497288227081299\n93.6399917602539 1.477353572845459\n83.35796356201172 1.0414211750030518\n91.53684997558594 1.6548962593078613\n96.76288604736328 1.1879498958587646\n89.65204620361328 1.6149208545684814\n80.90271759033203 1.8829424381256104\n85.37530517578125 1.702674150466919\n80.3994140625 1.4674813747406006\n84.2823486328125 1.4763164520263672\n96.10466766357422 1.0428643226623535\n91.8232650756836 1.146437406539917\n87.81658935546875 1.8529260158538818\n94.59446716308594 1.7889952659606934\n88.49081420898438 1.5795292854309082\n97.00790405273438 1.4620342254638672\n91.68185424804688 1.3564547300338745\n97.39276885986328 1.0309299230575562\n80.75239562988281 1.302687644958496\n87.96551513671875 1.2410178184509277\n91.6697006225586 1.8507330417633057\n95.40574645996094 1.3018969297409058\n96.36846160888672 1.2443230152130127\n80.5966567993164 1.650611400604248\n88.48704528808594 1.0398454666137695\n99.28273010253906 1.5687675476074219\n89.9427490234375 1.6616110801696777\n83.8802490234375 1.0650686025619507\n80.98805236816406 1.3144359588623047\n90.46640014648438 1.4533863067626953\n87.94342041015625 1.4379196166992188\n96.86217498779297 1.722522497177124\n97.1824951171875 1.7648491859436035\n94.94293975830078 1.7476022243499756\n88.57774353027344 1.9936397075653076\n81.87714385986328 1.3116981983184814\n84.58016967773438 1.3853662014007568\n90.22624969482422 1.8877133131027222\n93.26171875 1.3057349920272827\n85.26964569091797 1.7732207775115967\n88.49036407470703 1.4678995609283447\n83.45116424560547 1.398559808731079\n81.73490142822266 1.3599162101745605\n80.60186004638672 1.607072114944458\n92.12307739257812 1.3804558515548706\n82.58917236328125 1.9838064908981323\n96.34735107421875 1.0393333435058594\n92.86759948730469 1.4368032217025757\n84.02291107177734 1.7780158519744873\n80.03865814208984 1.6370843648910522\n85.43888854980469 1.7002469301223755\n95.39344787597656 1.953841209411621\n82.55075073242188 1.362874984741211\n89.55502319335938 1.2146159410476685\n91.37075805664062 1.6585370302200317\n93.4520034790039 1.9682751893997192\n89.2584228515625 1.5960586071014404\n96.02014923095703 1.2836705446243286\n87.00834655761719 1.1300926208496094\n91.74502563476562 1.676926612854004\n85.42420959472656 1.4379678964614868\n91.68942260742188 1.5796723365783691\n92.3317642211914 1.992002010345459\n93.67393493652344 1.5307087898254395\n82.57008361816406 1.2691798210144043\n90.5518569946289 1.7374107837677002\n96.5008316040039 1.7301230430603027\n95.30280303955078 1.5667967796325684\n96.31549072265625 1.2386984825134277\n86.32937622070312 1.1683495044708252\n93.25740814208984 1.2452566623687744\n88.98576354980469 1.1448354721069336\n91.8854751586914 1.7744994163513184\n85.98224639892578 1.1230238676071167\n93.2282485961914 1.1721405982971191\n95.08592987060547 1.9898439645767212\n87.02610778808594 1.5295970439910889\n87.21879577636719 1.9920413494110107\n89.9481430053711 1.9008828401565552\n93.08697509765625 1.6155953407287598\n88.41838073730469 1.7965482473373413\n89.20204162597656 1.735062599182129\n91.37265014648438 1.138883113861084\n94.8079605102539 1.4398438930511475\n99.36428833007812 1.8113322257995605\n88.27420043945312 1.9618552923202515\n83.03826904296875 1.1955100297927856\n96.51607513427734 1.683363676071167\n83.83062744140625 1.3281757831573486\n96.16497802734375 1.5947082042694092\n90.38520050048828 1.2728993892669678\n88.38111877441406 1.6290942430496216\n80.53921508789062 1.7611379623413086\n81.95087432861328 1.1960370540618896\n81.45306396484375 1.673715591430664\n91.85885620117188 1.1945915222167969\n81.97167205810547 1.0205057859420776\n81.46564483642578 1.524099588394165\n81.84530639648438 1.8652347326278687\n91.78020477294922 1.750521183013916\n92.25891876220703 1.0031507015228271\n86.03592681884766 1.903341293334961\n84.34727478027344 1.448254108428955\n91.22293853759766 1.0013437271118164\n85.41646575927734 1.6531617641448975\n88.41971588134766 1.967858910560608\n97.15705108642578 1.1303396224975586\n88.0723648071289 1.7646214962005615\n95.79047393798828 1.6941215991973877\n84.21778106689453 1.778570532798767\n84.01835632324219 1.653878927230835\n93.9107437133789 1.1549937725067139\n84.52613830566406 1.7052514553070068\n84.23381042480469 1.253612995147705\n98.05361938476562 1.1652004718780518\n87.08734130859375 1.5861719846725464\n84.90568542480469 1.189119577407837\n92.3504638671875 1.9187812805175781\n87.37548828125 1.3960113525390625\n84.04778289794922 1.3164339065551758\n92.84234619140625 1.783273696899414\n90.77714538574219 1.665143370628357\n97.57646942138672 1.3605051040649414\n81.65654754638672 1.3727788925170898\n81.46382141113281 1.6269168853759766\n92.97415161132812 1.4338579177856445\n92.45384216308594 1.4932732582092285\n98.71630859375 1.9220644235610962\n99.18801879882812 1.5020290613174438\n94.50535583496094 1.6706345081329346\n93.87782287597656 1.8368029594421387\n80.73177337646484 1.9445865154266357\n90.4026870727539 1.7702679634094238\n96.26884460449219 1.0647261142730713\n96.21143341064453 1.7131527662277222\n83.15489196777344 1.9959337711334229\n92.94298553466797 1.4255867004394531\n86.27571868896484 1.2139116525650024\n89.59510803222656 1.83095383644104\n98.43739318847656 1.6682121753692627\n81.6409912109375 1.2444205284118652\n90.85409545898438 1.7423512935638428\n93.0773696899414 1.0354089736938477\n84.70011138916016 1.230469822883606\n85.11830139160156 1.4981557130813599\n87.32134246826172 1.5977025032043457\n90.03490447998047 1.7946093082427979\n90.2147216796875 1.104181170463562\n89.17416381835938 1.6704487800598145\n83.1048355102539 1.7645598649978638\n82.71820831298828 1.4501047134399414\n94.20553588867188 1.3868703842163086\n99.91905212402344 1.7349451780319214\n94.77164459228516 1.1228876113891602\n81.1861343383789 1.5759849548339844\n90.24392700195312 1.784378170967102\n88.24333190917969 1.5038079023361206\n92.0295181274414 1.355574607849121\n92.56514739990234 1.2769356966018677\n85.94490814208984 1.9950015544891357\n86.05892181396484 1.0340492725372314\n91.34903717041016 1.6086660623550415\n81.53462982177734 1.8889412879943848\n82.08914184570312 1.0108835697174072\n82.99040985107422 1.5281376838684082\n94.89295959472656 1.6591260433197021\n88.80055236816406 1.74747896194458\n86.06137084960938 1.4740259647369385\n86.14717102050781 1.9505882263183594\n82.40860748291016 1.3710678815841675\n94.55815124511719 1.6016926765441895\n83.80721282958984 1.3749752044677734\n80.2197265625 1.3133662939071655\n82.26643371582031 1.3465780019760132\n84.53607940673828 1.9912424087524414\n83.35774230957031 1.839600920677185\n84.59407043457031 1.4957480430603027\n93.56404113769531 1.4851701259613037\n92.06135559082031 1.1837477684020996\n81.80078887939453 1.003249168395996\n86.82144927978516 1.3300600051879883\n88.28636932373047 1.1090788841247559\n91.64513397216797 1.5997440814971924\n99.58380889892578 1.9814856052398682\n80.24290466308594 1.8041205406188965\n98.6710205078125 1.2375580072402954\n84.29248046875 1.5288810729980469\n80.7286605834961 1.0446298122406006\n97.26927947998047 1.9321799278259277\n82.84217834472656 1.2857847213745117\n91.19561004638672 1.8906525373458862\n89.48432159423828 1.4106606245040894\n81.46939086914062 1.2941572666168213\n83.37738800048828 1.130700707435608\n88.78155517578125 1.6292293071746826\n85.92105102539062 1.5038135051727295\n98.54142761230469 1.201120138168335\n86.03743743896484 1.6141196489334106\n96.57463073730469 1.8828784227371216\n82.45696258544922 1.755530595779419\n94.83319091796875 1.4144964218139648\n87.38792419433594 1.395566701889038\n86.35507202148438 1.953106164932251\n94.02960968017578 1.4930729866027832\n98.34146881103516 1.3978049755096436\n94.55321502685547 1.219794511795044\n86.0677490234375 1.9958608150482178\n93.33192443847656 1.391430377960205\n84.49732971191406 1.7394044399261475\n99.59793090820312 1.265183448791504\n82.78313446044922 1.2995491027832031\n82.94720458984375 1.5027931928634644\n90.03015899658203 1.6642940044403076\n85.33958435058594 1.2336509227752686\n82.81128692626953 1.5233395099639893\n83.21725463867188 1.9979586601257324\n94.02526092529297 1.9433088302612305\n96.185546875 1.39166259765625\n94.62015533447266 1.422394037246704\n85.43770599365234 1.3459270000457764\n87.85009002685547 1.6309995651245117\n90.31221008300781 1.1308749914169312\n85.93450927734375 1.2944839000701904\n95.16260528564453 1.6092753410339355\n88.80064392089844 1.8153597116470337\n94.45771026611328 1.3654019832611084\n91.87638092041016 1.607332468032837\n92.98275756835938 1.7597867250442505\n82.830810546875 1.2815625667572021\n99.4336929321289 1.0294337272644043\n96.1424331665039 1.63151216506958\n91.74088287353516 1.8203094005584717\n94.86433410644531 1.6463167667388916\n85.03170776367188 1.8908147811889648\n81.66612243652344 1.9308836460113525\n97.11016845703125 1.944331407546997\n93.381103515625 1.7135376930236816\n88.73222351074219 1.8859648704528809\n94.90327453613281 1.683983325958252\n80.44949340820312 1.901411533355713\n97.26458740234375 1.1535134315490723\n88.45130920410156 1.220460057258606\n96.82727813720703 1.510488510131836\n84.75334167480469 1.760849952697754\n98.310791015625 1.583519697189331\n97.90435791015625 1.668187141418457\n89.99076843261719 1.2872053384780884\n94.72821044921875 1.4270225763320923\n93.0601577758789 1.5296818017959595\n94.30963897705078 1.1160366535186768\n93.94793701171875 1.6426379680633545\n86.19263458251953 1.08041512966156\n80.5118408203125 1.8654968738555908\n84.01466369628906 1.3280844688415527\n96.20509338378906 1.6256213188171387\n93.00784301757812 1.2826988697052002\n90.70428466796875 1.8038830757141113\n85.2476806640625 1.7395219802856445\n82.3729476928711 1.6036834716796875\n89.78605651855469 1.6776859760284424\n80.54756927490234 1.8309955596923828\n99.66976928710938 1.2932779788970947\n99.87591552734375 1.1250203847885132\n98.35208129882812 1.9846861362457275\n92.65155029296875 1.107607126235962\n89.141357421875 1.3425897359848022\n86.205078125 1.5620756149291992\n98.31776428222656 1.5817723274230957\n83.6128921508789 1.104830265045166\n82.37863159179688 1.130692720413208\n84.2057113647461 1.9208526611328125\n84.14820861816406 1.1117706298828125\n95.58302307128906 1.798877477645874\n99.08193969726562 1.109937310218811\n96.45347595214844 1.0028170347213745\n90.14459991455078 1.5874903202056885\n95.96649169921875 1.7088102102279663\n91.61677551269531 1.8787145614624023\n90.53985595703125 1.1856184005737305\n86.97960662841797 1.5280441045761108\n80.56758117675781 1.1526362895965576\n85.12653350830078 1.779416561126709\n80.04230499267578 1.4652273654937744\n91.02215576171875 1.9656496047973633\n88.33577728271484 1.4623045921325684\n98.91292572021484 1.866753101348877\n94.00769805908203 1.5757315158843994\n92.6316909790039 1.8505401611328125\n84.2348403930664 1.1799921989440918\n95.00141906738281 1.9301543235778809\n90.13406372070312 1.0038076639175415\n88.1715316772461 1.2157790660858154\n93.29283142089844 1.9890989065170288\n89.0271224975586 1.7377375364303589\n91.8408203125 1.7466797828674316\n99.33059692382812 1.4390294551849365\n98.48324584960938 1.758401870727539\n83.00625610351562 1.3527941703796387\n98.83973693847656 1.079725742340088\n92.71626281738281 1.8784775733947754\n99.38002014160156 1.965032696723938\n95.19747161865234 1.8720885515213013\n80.86019897460938 1.009787917137146\n87.3360366821289 1.2022541761398315\n81.04074096679688 1.8308234214782715\n98.82062530517578 1.1745812892913818\n90.1404800415039 1.9698154926300049\n80.95049285888672 1.7338175773620605\n91.59555053710938 1.0537080764770508\n92.85134887695312 1.5899709463119507\n89.0922622680664 1.7719515562057495\n87.76759338378906 1.7769479751586914\n94.38178253173828 1.0224690437316895\n98.71640014648438 1.6177469491958618\n89.71712493896484 1.4378740787506104\n83.92728424072266 1.8954205513000488\n83.64366912841797 1.7923521995544434\n98.96711730957031 1.6632966995239258\n96.44139862060547 1.7354342937469482\n81.7240219116211 1.1570839881896973\n86.23362731933594 1.757424235343933\n86.37788391113281 1.4433300495147705\n80.74813842773438 1.4932761192321777\n97.84660339355469 1.9760985374450684\n93.23468780517578 1.0188976526260376\n84.22872924804688 1.3363087177276611\n96.82887268066406 1.221339225769043\n81.30179595947266 1.9991141557693481\n84.09915161132812 1.252927303314209\n90.10759735107422 1.9463887214660645\n93.94319915771484 1.526930809020996\n94.18348693847656 1.9327185153961182\n84.27458190917969 1.7738151550292969\n93.13091278076172 1.7500479221343994\n80.47447967529297 1.9000530242919922\n98.79817199707031 1.4553866386413574\n86.2924575805664 1.3707048892974854\n88.50386047363281 1.667838215827942\n88.3800048828125 1.0530340671539307\n91.60956573486328 1.302704095840454\n86.78005981445312 1.4603569507598877\n90.30412292480469 1.517063856124878\n84.20452880859375 1.890026330947876\n89.8901596069336 1.0054442882537842\n93.95210266113281 1.5633063316345215\n99.6352310180664 1.0056164264678955\n98.12481689453125 1.6828978061676025\n87.24083709716797 1.657820224761963\n88.11785888671875 1.1198830604553223\n94.37049865722656 1.6802012920379639\n82.21324157714844 1.0426301956176758\n90.2756576538086 1.7751855850219727\n97.32678985595703 1.5809354782104492\n80.03449249267578 1.877395510673523\n90.29203796386719 1.1785944700241089\n94.00375366210938 1.9306368827819824\n87.07160186767578 1.0483334064483643\n93.4297866821289 1.231441855430603\n93.20204162597656 1.9675328731536865\n87.07601165771484 1.1881678104400635\n82.18881225585938 1.1132152080535889\n95.82549285888672 1.9563894271850586\n89.05828857421875 1.929946780204773\n80.41268157958984 1.9739079475402832\n88.15396118164062 1.0157376527786255\n94.43635559082031 1.439072847366333\n80.95018005371094 1.2114360332489014\n97.67599487304688 1.9241340160369873\n97.97877502441406 1.38897705078125\n82.70357513427734 1.4984066486358643\n80.9278335571289 1.8461503982543945\n82.1534194946289 1.3695964813232422\n96.12203979492188 1.37688410282135\n84.20735168457031 1.0404396057128906\n89.20557403564453 1.8401185274124146\n85.48104858398438 1.667121410369873\n96.72209167480469 1.595235824584961\n83.95387268066406 1.9338353872299194\n89.39398956298828 1.9475802183151245\n87.39876556396484 1.826859474182129\n84.99310302734375 1.478555679321289\n95.20046997070312 1.8166275024414062\n84.63009643554688 1.2446922063827515\n90.1427001953125 1.2517023086547852\n92.98526763916016 1.2036794424057007\n94.19744873046875 1.0349105596542358\n95.95453643798828 1.812288522720337\n82.15237426757812 1.9743807315826416\n92.40218353271484 1.7967054843902588\n89.28179931640625 1.2706964015960693\n94.41395568847656 1.0045077800750732\n93.30809020996094 1.5501983165740967\n93.10982513427734 1.1014915704727173\n85.00166320800781 1.9952356815338135\n94.99205780029297 1.593123197555542\n92.78434753417969 1.5873770713806152\n85.2752685546875 1.8876330852508545\n96.14407348632812 1.6973540782928467\n96.59182739257812 1.777432918548584\n85.21195983886719 1.247289776802063\n89.11259460449219 1.9683427810668945\n84.28923797607422 1.442199945449829\n95.35619354248047 1.0686843395233154\n81.9161376953125 1.6271142959594727\n98.88188171386719 1.9269213676452637\n94.53003692626953 1.381432294845581\n80.15739440917969 1.875699758529663\n99.72911071777344 1.9995348453521729\n88.91797637939453 1.2367712259292603\n94.02236938476562 1.9013264179229736\n92.25674438476562 1.1762142181396484\n87.81084442138672 1.028839349746704\n93.73471069335938 1.3626747131347656\n85.55268096923828 1.481982707977295\n88.40141296386719 1.7987523078918457\n81.53286743164062 1.2459611892700195\n95.06607055664062 1.9266705513000488\n98.91923522949219 1.6137280464172363\n86.81133270263672 1.1890655755996704\n85.51641082763672 1.8769354820251465\n95.20496368408203 1.021319031715393\n91.9203872680664 1.3852016925811768\n90.42877197265625 1.4692349433898926\n93.15274810791016 1.7049617767333984\n92.43025207519531 1.815345048904419\n90.53237915039062 1.2616076469421387\n97.911376953125 1.6995875835418701\n99.65914916992188 1.1845998764038086\n91.41571807861328 1.7571431398391724\n86.17327117919922 1.4590321779251099\n82.06706237792969 1.583986520767212\n81.95830535888672 1.8748775720596313\n89.40499114990234 1.8530055284500122\n87.00450134277344 1.7186863422393799\n90.86737060546875 1.1591532230377197\n87.36505889892578 1.1975839138031006\n86.66249084472656 1.802229881286621\n91.9198989868164 1.3777878284454346\n80.39051818847656 1.9387800693511963\n87.5408935546875 1.2266218662261963\n92.21771240234375 1.302396297454834\n89.4444580078125 1.4973241090774536\n82.5759048461914 1.4075777530670166\n93.01850128173828 1.1784536838531494\n87.36629486083984 1.3914499282836914\n93.0128173828125 1.9193010330200195\n99.21200561523438 1.8135334253311157\n82.42012023925781 1.9386579990386963\n91.3363037109375 1.080588698387146\n84.02668762207031 1.849124550819397\n85.27171325683594 1.414588451385498\n84.32006072998047 1.6265616416931152\n90.93821716308594 1.0528053045272827\n81.02252960205078 1.7647711038589478\n83.12752532958984 1.9332249164581299\n80.77437591552734 1.6287462711334229\n81.12611389160156 1.7884021997451782\n96.69781494140625 1.6648938655853271\n82.15121459960938 1.7358323335647583\n85.9214859008789 1.6570940017700195\n84.7430648803711 1.4608850479125977\n95.41434478759766 1.7912076711654663\n82.72222900390625 1.8343868255615234\n87.48568725585938 1.8611987829208374\n86.147705078125 1.5697367191314697\n81.70919799804688 1.4862655401229858\n87.0477066040039 1.3368690013885498\n98.66348266601562 1.7423510551452637\n96.37520599365234 1.3208246231079102\n92.814453125 1.8332717418670654\n80.84992218017578 1.9198791980743408\n87.80859375 1.0776509046554565\n84.58948516845703 1.8839328289031982\n89.78652954101562 1.70749831199646\n81.93563842773438 1.118016242980957\n84.79975891113281 1.4021191596984863\n90.56642150878906 1.2989983558654785\n82.07353210449219 1.1830602884292603\n82.82074737548828 1.949235200881958\n\n\nText(0, 0.5, 'Electricity Consumption')\n\n\n\n\n\n\ntorch.distributions.Uniform(0, 1).support\n\nInterval(lower_bound=0.0, upper_bound=1.0)\n\n\n\ntorch.distributions.LogNormal(0, 1).support\n\nGreaterThan(lower_bound=0.0)\n\n\n\nforward(xs, theta_0, theta_1)\n\ntensor([83.7934])\n\n\n\ntorch.stack(preds).max(axis=0).values\n\ntensor([ 100.2101,  118.9044,  137.5986,  156.2929,  175.8384,  195.5405,\n         215.2426,  234.9448,  254.6469,  274.3490,  294.0511,  313.7533,\n         333.4554,  353.1575,  372.8596,  392.5617,  412.2639,  431.9660,\n         451.6681,  471.3702,  491.0724,  510.7745,  530.4766,  550.1788,\n         569.8809,  589.5830,  609.2852,  628.9872,  648.6894,  668.3915,\n         688.0936,  707.7958,  727.4979,  747.2000,  766.9022,  786.6043,\n         806.3064,  826.0085,  845.7106,  865.4127,  885.1149,  904.8170,\n         924.5192,  944.2213,  963.9234,  983.6255, 1003.3276, 1023.0298,\n        1042.7319, 1062.4340, 1082.1361, 1101.8383, 1121.5404, 1141.2424,\n        1160.9446, 1180.6467, 1200.3489, 1220.0510, 1239.7531, 1259.4552,\n        1279.1573, 1298.8595, 1318.5615, 1338.2638, 1357.9658, 1377.6680,\n        1397.3701, 1417.0723, 1436.7743, 1456.4764, 1476.1786, 1495.8806,\n        1515.5829, 1535.2849, 1554.9871, 1574.6892, 1594.3914, 1614.0935,\n        1633.7955, 1653.4977, 1673.1998, 1692.9020, 1712.6040, 1732.3063,\n        1752.0083, 1771.7104, 1791.4126, 1811.1147, 1830.8168, 1850.5189,\n        1870.2211, 1889.9231, 1909.6254, 1929.3274, 1949.0295, 1968.7317,\n        1988.4338, 2008.1359, 2027.8381, 2047.5402])"
  },
  {
    "objectID": "quizzes.html",
    "href": "quizzes.html",
    "title": "Quizzes",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ES 661 Probabilistic Machine Learning",
    "section": "",
    "text": "Summary\n\nInstructor: Nipun Batra (nipun.batra@iitgn.ac.in)\nTeaching Assistants: Zeel B Patel, Sarth Dubey, Madhav Kanda, Haikoo Khandor\nCourse Timings: Monday 330-450 PM IST and Thursday 2-3:20 PM IST\nSlack Invite corrected!\n\n\n\nMain topics\n\nBayesian Inference\nEstimation: Maximum Likelihood, Maximum a Posteriori, Full Bayesian\nSampling: Rejection Sampling, Monte Carlo, Specific Sampling Techniques (like Box Muller)\nApproximate Inference: Variational Inference, Markov Chain Monte Carlo, Laplace Approximation\nModels: Bayesian Linear, Logistic regression; Bayesian Neural Networks; Gaussian Processes; Probabilistic PCA\nApplications: Bayesian Optimization, Active learning"
  }
]