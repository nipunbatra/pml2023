---
title: Assignment 4 (released 28 Oct, due 8 Nov 7 PM)
date: 2023-10-28
---

## Instructions

- Total Marks: 9
- Use torch for the assignment.
- For sampling, use torch.distributions and do not use torch.random directly.
- The assignment has to be done in groups of two.
- The assignment should be a single jupyter notebook. 
- The results from every question of your assignment should be in visual formats such as plots and tables. Don't show model's log directly in Viva. All plots should have labels and legends appropriately. If not done, we may cut some marks for presentation (e.g. 10%).

## Questions 

1. [1 mark] Implement Logistic Regression using the Pyro library referring [[1]](https://docs.pyro.ai/en/stable/) for guidance. Show both the mean prediction as well as standard deviation in the predictions over the 2d grid. Use NUTS MCMC sampling to sample the posterior. Take 1000 samples for posterior distribution and use 500 samples as burn/warm up. Use the below given dataset.
```py
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split

X, y = make_moons(n_samples=100, noise=0.3, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=42)
```
2. [2 marks] Consider the FVC dataset example discussed in the class. Find the notebook link at [[2]]((https://github.com/nipunbatra/pml-teaching/blob/master/notebooks/hierarchical_lr_numpyro.ipynb)). We had only used the train dataset. Now, we want to find out the performance of various models on the test dataset. Use the given dataset and deduce which model works best in terms of error (`MAE`) and coverage? The base model is Linear Regression by Sklearn (```from sklearn.linear_model import LinearRegression```). Plot the trace diagrams and posterior distribution. Also plot the predictive posterior distribution with 90% confidence interval. 
```py
x_train, x_test, y_train, y_test = train_test_split(train["Weeks"], train['FVC'], train_size = 0.8, random_state = 0)
```
Pooled Model
\textbf{Pooled:}
\begin{align*}
\alpha &\sim \mathcal{N}(0, 500) \\
\beta &\sim \mathcal{N}(0, 500) \\
\sigma &\sim \text{HalfNormal}(100)
\end{align*}
\begin{equation*}
FVC_i \sim \text{Normal}(\alpha + \beta \cdot Week_i, \sigma)
\end{equation*}

Partially pooled model with the same sigma
\textbf{Partially pooled (with same sigma):}
\begin{align*}
\mu_\alpha &\sim \mathcal{N}(0, 500) \\
\sigma_\alpha &\sim \text{HalfNormal}(100) \\
\mu_\beta &\sim \mathcal{N}(0, 3) \\
\sigma_\beta &\sim \text{HalfNormal}(3) \\
\alpha_i &\sim \mathcal{N}(\mu_\alpha, \sigma_\alpha) \\
\beta_i &\sim \mathcal{N}(\mu_\beta, \sigma_\beta) \\
\sigma &\sim \text{HalfNormal}(100)
\end{align*}

\begin{equation*}
FVC_j \sim \text{Normal}(\alpha_i + \beta_i \cdot Week_j, \sigma)
\end{equation*}

Partially pooled model with sigma hyperpriors
\textbf{Partially pooled with sigma hyperpriors:}
\begin{align*}
\mu_\alpha &\sim \mathcal{N}(0, 500) \\
\sigma_\alpha &\sim \text{HalfNormal}(100) \\
\mu_\beta &\sim \mathcal{N}(0, 3) \\
\sigma_\beta &\sim \text{HalfNormal}(3) \\
\alpha_i &\sim \mathcal{N}(\mu_\alpha, \sigma_\alpha) \\
\beta_i &\sim \mathcal{N}(\mu_\beta, \sigma_\beta) \\
\gamma_\sigma &\sim \text{HalfNormal}(30) \\
\sigma_i &\sim \text{Exp}(\gamma_\sigma)
\end{align*}

\begin{equation*}
FVC_j \sim \text{Normal}(\alpha_i + \beta_i \cdot Week_j, \sigma_i)
\end{equation*}

3. [3 marks] Use your version of following models to reproduce figure 4 from the paper referenced at [[2]](http://proceedings.mlr.press/v80/garnelo18a/garnelo18a.pdf). You can also refer to the notebook in the course.
    i. Hypernet [1.5 marks]
    ii. Neural Processes [1.5 marks]

4. [3 marks] Write the Random walk Metropolis Hastings algorithms from scratch. Take 1000 samples using below given log probs and compare the mean and covariance matrix with hamiltorch's standard HMC and [emceeâ€™s](https://emcee.readthedocs.io) Metropolis Hastings implementation. Use 500 samples as the burn/warm up samples. Also check the relation between acceptance ratio and the sigma of the proposal distribution in your from scratch implementation. Use the log likelihood function given below. 

```py
import torch.distributions as D
def log_likelihood(omega):
    omega = torch.tensor(omega)
    mean = torch.tensor([0., 0.])
    stddev = torch.tensor([0.5, 1.])
    return D.MultivariateNormal(mean, torch.diag(stddev**2)).log_prob(omega).sum()
```
