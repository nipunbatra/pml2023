---
title: Assignment 4 (released 27 Sep, due 10 October 7 PM)
date: 2023-10-27
---

## Instructions

- Total Marks: 9
- Use torch for the assignment.
- For sampling, use torch.distributions and do not use torch.random directly.
- The assignment has to be done in groups of two.
- The assignment should be a single jupyter notebook. 
- The results from every question of your assignment should be in visual formats such as plots and tables. Don't show model's log directly in Viva. All plots should have labels and legends appropriately. If not done, we may cut some marks for presentation (e.g. 10%).

## Questions 

1. [1 mark] Implement Logistic Regression using the Pyro library referring [1] for guidance. Show both the mean prediction as well as standard deviation in the predictions. Use the below given dataset.
[[1]](https://docs.pyro.ai/en/stable/)
```py
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split

X, y = make_moons(n_samples=100, noise=0.3, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=42)
```
2. [2 marks] Consider the FVC dataset example discussed in the class. Find the notebook link at [2]. We only had the train dataset. Now, we want to find out the performance of various models on the test dataset. Use the given dataset and deduce which model works best in terms of accuracy and coverage? The base model is Linear Regression by Sklearn (```from sklearn.linear_model import LinearRegression```). Plot the trace diagrams and posterior distribution. Also plot the predictive posteriror distribution with 90% confidence interval. 
[[2]](https://github.com/nipunbatra/pml-teaching/blob/master/notebooks/hierarchical_lr_numpyro.ipynb)
```py
x_train, x_test, y_train, y_test = train_test_split(train["Weeks"], train['FVC'], train_size = 0.8, random_state = 0)
```
Pooled Model
\textbf{Pooled:}
\begin{align*}
\alpha &\sim \mathcal{N}(0, 500) \\
\beta &\sim \mathcal{N}(0, 500) \\
\sigma &\sim \text{HalfNormal}(100)
\end{align*}

Partially pooled model with the same sigma
\textbf{Partially pooled (with same sigma):}
\begin{align*}
\mu_\alpha &\sim \mathcal{N}(0, 500) \\
\sigma_\alpha &\sim \text{HalfNormal}(100) \\
\mu_\beta &\sim \mathcal{N}(0, 3) \\
\sigma_\beta &\sim \text{HalfNormal}(3) \\
\alpha_i &\sim \mathcal{N}(\mu_\alpha, \sigma_\alpha) \\
\beta_i &\sim \mathcal{N}(\mu_\beta, \sigma_\beta) \\
\sigma &\sim \text{HalfNormal}(100)
\end{align*}

Partially pooled model with Sigma Hyperpriors
\textbf{Partially pooled with sigma hyperpriors:}
\begin{align*}
\mu_\alpha &\sim \mathcal{N}(0, 500) \\
\sigma_\alpha &\sim \text{HalfNormal}(100) \\
\mu_\beta &\sim \mathcal{N}(0, 3) \\
\sigma_\beta &\sim \text{HalfNormal}(3) \\
\alpha_i &\sim \mathcal{N}(\mu_\alpha, \sigma_\alpha) \\
\beta_i &\sim \mathcal{N}(\mu_\beta, \sigma_\beta) \\
\mu_\sigma &\sim \mathcal{N}(0, 10) \\
\sigma_\sigma &\sim \text{HalfNormal}(30) \\
\gamma_\sigma &\sim \text{LogNormal}(\mu_\sigma, \sigma_\sigma) \\
\sigma_i &\sim \text{HalfNormal}(\gamma_\sigma)
\end{align*}

3. [3 marks] Use your version of following models to reproduce figure 4 from the paper referenced at [2] You can also refer to the notebook at [3].
[[2]](http://proceedings.mlr.press/v80/garnelo18a/garnelo18a.pdf)
[[3]](https://colab.research.google.com/drive/1i8gMWybkVhF66hqbjrSkbkMo54dIOJsx)
    i. Hypernet [1.5 marks]
    ii. Neural Processes [1.5 marks]

4. [3 marks] Write the Random walk Metropolis Hastings algorithms from scratch. Take 1000 samples using below given log probs and compare the mean and covariance matrix with hamiltorch and emceeâ€™s implementation. Use 500 samples as the burn/warm up samples. Also check the relation between acceptance ratio and the proposed sigma. Use the log likelihood function given below. 

```py
import torch.distributions as D
def log_likelihood(omega):
    omega = torch.tensor(omega)
    mean = torch.tensor([0., 0.])
    stddev = torch.tensor([0.5, 1.])
    return D.MultivariateNormal(mean, torch.diag(stddev**2)).log_prob(omega).sum()
```
